{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9614c7f",
   "metadata": {},
   "source": [
    "# An Introduction to JAX\n",
    "\n",
    "\n",
    "```{include} _admonition/gpu.md\n",
    "```\n",
    "\n",
    "This lecture provides a short introduction to [Google JAX](https://github.com/google/jax).\n",
    "\n",
    "As mentioned above, the lecture was built using a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3afa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4fa24",
   "metadata": {},
   "source": [
    "## JAX as a NumPy Replacement\n",
    "\n",
    "\n",
    "One way to use JAX is as a plug-in NumPy replacement. Let's look at the\n",
    "similarities and differences.\n",
    "\n",
    "### Similarities\n",
    "\n",
    "\n",
    "The following import is standard, replacing `import numpy as np`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04170f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa0752",
   "metadata": {},
   "source": [
    "Now we can use `jnp` in place of `np` for the usual array operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.asarray((1.0, 3.2, -1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d482aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89681afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e04267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.dot(a, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf57bc2",
   "metadata": {},
   "source": [
    "However, the array object `a` is not a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a909c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9ecee",
   "metadata": {},
   "source": [
    "Even scalar-valued maps on arrays return JAX arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17336703",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8039e",
   "metadata": {},
   "source": [
    "JAX arrays are also called \"device arrays,\" where term \"device\" refers to a\n",
    "hardware accelerator (GPU or TPU).\n",
    "\n",
    "(In the terminology of GPUs, the \"host\" is the machine that launches GPU operations, while the \"device\" is the GPU itself.)\n",
    "\n",
    "\n",
    "\n",
    "Operations on higher dimensional arrays are also similar to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.ones((2, 2))\n",
    "B = jnp.identity(2)\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fab618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.inv(B)   # Inverse of identity is identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaff7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.eigh(B)  # Computes eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e03765",
   "metadata": {},
   "source": [
    "### Differences\n",
    "\n",
    "\n",
    "One difference between NumPy and JAX is that JAX currently uses 32 bit floats by default.  \n",
    "\n",
    "This is standard for GPU computing and can lead to significant speed gains with small loss of precision.\n",
    "\n",
    "However, for some calculations precision matters.  In these cases 64 bit floats can be enforced via the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269f1cb",
   "metadata": {},
   "source": [
    "Let's check this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.ones(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51a433",
   "metadata": {},
   "source": [
    "As a NumPy replacement, a more significant difference is that arrays are treated as **immutable**.  \n",
    "\n",
    "For example, with NumPy we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.linspace(0, 1, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52931505",
   "metadata": {},
   "source": [
    "and then mutate the data in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa2551",
   "metadata": {},
   "source": [
    "In JAX this fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffba904",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.linspace(0, 1, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52608ff",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "a[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5b181",
   "metadata": {},
   "source": [
    "In line with immutability, JAX does not support inplace operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array((2, 1))\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330136dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.array((2, 1))\n",
    "a_new = a.sort()\n",
    "a, a_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2de260",
   "metadata": {},
   "source": [
    "The designers of JAX chose to make arrays immutable because JAX uses a\n",
    "functional programming style.  More on this below.  \n",
    "\n",
    "Note that, while mutation is discouraged, it is in fact possible with `at`, as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf314a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.linspace(0, 1, 3)\n",
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7731cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.at[0].set(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234e093",
   "metadata": {},
   "source": [
    "We can check that the array is mutated by verifying its identity is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b758d9",
   "metadata": {},
   "source": [
    "## Random Numbers\n",
    "\n",
    "Random numbers are also a bit different in JAX, relative to NumPy.  Typically, in JAX, the state of the random number generator needs to be controlled explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e73c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d97913",
   "metadata": {},
   "source": [
    "First we produce a key, which seeds the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b28883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edefb7d1",
   "metadata": {},
   "source": [
    "Now we can use the key to generate some random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.normal(key, (3, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172582ff",
   "metadata": {},
   "source": [
    "If we use the same key again, we initialize at the same seed, so the random numbers are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a02c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, (3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b4143",
   "metadata": {},
   "source": [
    "To produce a (quasi-) independent draw, best practice is to \"split\" the existing key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db78238",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(subkey, (3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd8760",
   "metadata": {},
   "source": [
    "The function below produces `k` (quasi-) independent random `n x n` matrices using this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5584741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_matrices(key, n, k):\n",
    "    matrices = []\n",
    "    for _ in range(k):\n",
    "        key, subkey = random.split(key)\n",
    "        matrices.append(random.uniform(subkey, (n, n)))\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89090bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = gen_random_matrices(key, 2, 2)\n",
    "for A in matrices:\n",
    "    print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc169161",
   "metadata": {},
   "source": [
    "One point to remember is that JAX expects tuples to describe array shapes, even for flat arrays.  Hence, to get a one-dimensional array of normal random draws we use `(len, )` for the shape, as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, (5, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a6d49",
   "metadata": {},
   "source": [
    "## JIT compilation\n",
    "\n",
    "The JAX just-in-time (JIT) compiler accelerates logic within functions by fusing linear\n",
    "algebra operations into a single optimized kernel that the host can\n",
    "launch on the GPU / TPU (or CPU if no accelerator is detected).\n",
    "\n",
    "### A first example\n",
    "\n",
    "To see the JIT compiler in action, consider the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06554faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    a = 3*x + jnp.sin(x) + jnp.cos(x**2) - jnp.cos(2*x) - x**2 * 0.4 * x**1.5\n",
    "    return jnp.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fcd23",
   "metadata": {},
   "source": [
    "Let's build an array to call the function on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50_000_000\n",
    "x = jnp.ones(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b85c5",
   "metadata": {},
   "source": [
    "How long does the function take to execute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2a38c",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here, in order to measure actual speed, we use the `block_until_ready()` method \n",
    "to hold the interpreter until the results of the computation are returned from\n",
    "the device. This is necessary because JAX uses asynchronous dispatch, which\n",
    "allows the Python interpreter to run ahead of GPU computations.\n",
    "\n",
    "```\n",
    "\n",
    "The code doesn't run as fast as we might hope, given that it's running on a GPU.\n",
    "\n",
    "But if we run it a second time it becomes much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9472c",
   "metadata": {},
   "source": [
    "This is because the built in functions like `jnp.cos` are JIT compiled and the\n",
    "first run includes compile time.\n",
    "\n",
    "Why would JAX want to JIT-compile built in functions like `jnp.cos` instead of\n",
    "just providing pre-compiled versions, like NumPy?\n",
    "\n",
    "The reason is that the JIT compiler can specialize on the *size* of the array\n",
    "being used, which is helpful for parallelization.\n",
    "\n",
    "For example, in running the code above, the JIT compiler produced a version of `jnp.cos` that is\n",
    "specialized to floating point arrays of size `n = 50_000_000`.\n",
    "\n",
    "We can check this by calling `f` with a new array of different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 50_000_001\n",
    "y = jnp.ones(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f(y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2f071",
   "metadata": {},
   "source": [
    "Notice that the execution time increases, because now new versions of \n",
    "the built-ins like `jnp.cos` are being compiled, specialized to the new array\n",
    "size.\n",
    "\n",
    "If we run again, the code is dispatched to the correct compiled version and we\n",
    "get faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f(y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a335f",
   "metadata": {},
   "source": [
    "The compiled versions for the previous array size are still available in memory\n",
    "too, and the following call is dispatched to the correct compiled code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e427b8",
   "metadata": {},
   "source": [
    "### Compiling the outer function\n",
    "\n",
    "We can do even better if we manually JIT-compile the outer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jit = jax.jit(f)   # target for JIT compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34c859",
   "metadata": {},
   "source": [
    "Let's run once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95e787",
   "metadata": {},
   "source": [
    "And now let's time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33c3a2",
   "metadata": {},
   "source": [
    "Note the speed gain.\n",
    "\n",
    "This is because the array operations are fused and no intermediate arrays are created.\n",
    "\n",
    "\n",
    "Incidentally, a more common syntax when targetting a function for the JIT\n",
    "compiler is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138efa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    a = 3*x + jnp.sin(x) + jnp.cos(x**2) - jnp.cos(2*x) - x**2 * 0.4 * x**1.5\n",
    "    return jnp.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53667b3",
   "metadata": {},
   "source": [
    "## Functional Programming\n",
    "\n",
    "From JAX's documentation:\n",
    "\n",
    "*When walking about the countryside of Italy, the people will not hesitate to tell you that JAX has “una anima di pura programmazione funzionale”.*\n",
    "\n",
    "\n",
    "In other words, JAX assumes a functional programming style.\n",
    "\n",
    "The major implication is that JAX functions should be pure.\n",
    "    \n",
    "A pure function will always return the same result if invoked with the same inputs.\n",
    "\n",
    "In particular, a pure function has\n",
    "\n",
    "* no dependence on global variables and\n",
    "* no side effects\n",
    "\n",
    "\n",
    "JAX will not usually throw errors when compiling impure functions but execution becomes unpredictable.\n",
    "\n",
    "Here's an illustration of this fact, using global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1  # global\n",
    "\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    return a + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a150d1b",
   "metadata": {},
   "source": [
    "In the code above, the global value `a=1` is fused into the jitted function.\n",
    "\n",
    "Even if we change `a`, the output of `f` will not be affected --- as long as the same compiled version is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8411c5",
   "metadata": {},
   "source": [
    "Changing the dimension of the input triggers a fresh compilation of the function, at which time the change in the value of `a` takes effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87938925",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee531343",
   "metadata": {},
   "source": [
    "Moral of the story: write pure functions when using JAX!\n",
    "\n",
    "\n",
    "## Gradients\n",
    "\n",
    "JAX can use automatic differentiation to compute gradients.\n",
    "\n",
    "This can be extremely useful for optimization and solving nonlinear systems.\n",
    "\n",
    "We will see significant applications later in this lecture series.\n",
    "\n",
    "For now, here's a very simple illustration involving the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3deda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad391bb5",
   "metadata": {},
   "source": [
    "Let's take the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba403ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime(10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da58ffb",
   "metadata": {},
   "source": [
    "Let's plot the function and derivative, noting that $f'(x) = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b80dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_grid = jnp.linspace(-4, 4, 200)\n",
    "ax.plot(x_grid, f(x_grid), label=\"$f$\")\n",
    "ax.plot(x_grid, [f_prime(x) for x in x_grid], label=\"$f'$\")\n",
    "ax.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed703f2",
   "metadata": {},
   "source": [
    "## Writing vectorized code\n",
    "\n",
    "Writing fast JAX code requires shifting repetitive tasks from loops to array processing operations, so that the JAX compiler can easily understand the whole operation and generate more efficient machine code.\n",
    "\n",
    "This procedure is called **vectorization** or **array programming**, and will be familiar to anyone who has used NumPy or MATLAB.\n",
    "\n",
    "In most ways, vectorization is the same in JAX as it is in NumPy.\n",
    "\n",
    "But there are also some differences, which we highlight here.\n",
    "\n",
    "As a running example, consider the function\n",
    "\n",
    "$$\n",
    "    f(x,y) = \\frac{\\cos(x^2 + y^2)}{1 + x^2 + y^2}\n",
    "$$\n",
    "\n",
    "Suppose that we want to evaluate this function on a square grid of $x$ and $y$ points and then plot it.\n",
    "\n",
    "To clarify, here is the slow `for` loop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x, y):\n",
    "    return jnp.cos(x**2 + y**2) / (1 + x**2 + y**2)\n",
    "\n",
    "n = 80\n",
    "x = jnp.linspace(-2, 2, n)\n",
    "y = x\n",
    "\n",
    "z_loops = np.empty((n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        z_loops[i, j] = f(x[i], y[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3877d",
   "metadata": {},
   "source": [
    "Even for this very small grid, the run time is extremely slow.\n",
    "\n",
    "(Notice that we used a NumPy array for `z_loops` because we wanted to write to it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a090d99",
   "metadata": {},
   "source": [
    "OK, so how can we do the same operation in vectorized form?\n",
    "\n",
    "If you are new to vectorization, you might guess that we can simply write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223302d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bad = f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c80cb4",
   "metadata": {},
   "source": [
    "But this gives us the wrong result because JAX doesn't understand the nested for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a22dfd",
   "metadata": {},
   "source": [
    "Here is what we actually wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d51b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_loops.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd01e8",
   "metadata": {},
   "source": [
    "To get the right shape and the correct nested for loop calculation, we can use a `meshgrid` operation designed for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e338eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mesh, y_mesh = jnp.meshgrid(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09f035",
   "metadata": {},
   "source": [
    "Now we get what we want and the execution time is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_mesh = f(x_mesh, y_mesh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_mesh = f(x_mesh, y_mesh) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db15502",
   "metadata": {},
   "source": [
    "Let's confirm that we got the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.allclose(z_mesh, z_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13011e84",
   "metadata": {},
   "source": [
    "Now we can set up a serious grid and run the same calculation (on the larger grid) in a short amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f227e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6000\n",
    "x = jnp.linspace(-2, 2, n)\n",
    "y = x\n",
    "x_mesh, y_mesh = jnp.meshgrid(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13481711",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_mesh = f(x_mesh, y_mesh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_mesh = f(x_mesh, y_mesh) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ce13c",
   "metadata": {},
   "source": [
    "But there is one problem here: the mesh grids use a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f358609",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mesh.nbytes + y_mesh.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a53abe",
   "metadata": {},
   "source": [
    "By comparison, the flat array `x` is just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b910c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.nbytes  # and y is just a pointer to x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c824038",
   "metadata": {},
   "source": [
    "This extra memory usage can be a big problem in actual research calculations.\n",
    "\n",
    "So let's try a different approach using [jax.vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d60f7f",
   "metadata": {},
   "source": [
    "First we vectorize `f` in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e525d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vec_y = jax.vmap(f, in_axes=(None, 0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c1325",
   "metadata": {},
   "source": [
    "In the line above, `(None, 0)` indicates that we are vectorizing in the second argument, which is `y`.\n",
    "\n",
    "Next, we vectorize in the first argument, which is `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vec = jax.vmap(f_vec_y, in_axes=(0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bf997",
   "metadata": {},
   "source": [
    "With this construction, we can now call the function $f$ on flat (low memory) arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b096001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_vmap = f_vec(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbcc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_vmap = f_vec(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d0407",
   "metadata": {},
   "source": [
    "The execution time is essentially the same as the mesh operation but we are using much less memory.\n",
    "\n",
    "And we produce the correct answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.allclose(z_vmap, z_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960e0f2",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "```{exercise-start}\n",
    ":label: jax_intro_ex2\n",
    "```\n",
    "\n",
    "In the Exercise section of [a lecture on Numba and parallelization](https://python-programming.quantecon.org/parallelization.html), we used Monte Carlo to price a European call option.\n",
    "\n",
    "The code was accelerated by Numba-based multithreading.\n",
    "\n",
    "Try writing a version of this operation for JAX, using all the same\n",
    "parameters.\n",
    "\n",
    "If you are running your code on a GPU, you should be able to achieve\n",
    "significantly faster execution.\n",
    "\n",
    "\n",
    "```{exercise-end}\n",
    "```\n",
    "\n",
    "\n",
    "```{solution-start} jax_intro_ex2\n",
    ":class: dropdown\n",
    "```\n",
    "Here is one solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10_000_000\n",
    "\n",
    "n, β, K = 20, 0.99, 100\n",
    "μ, ρ, ν, S0, h0 = 0.0001, 0.1, 0.001, 10, 0\n",
    "\n",
    "@jax.jit\n",
    "def compute_call_price_jax(β=β,\n",
    "                           μ=μ,\n",
    "                           S0=S0,\n",
    "                           h0=h0,\n",
    "                           K=K,\n",
    "                           n=n,\n",
    "                           ρ=ρ,\n",
    "                           ν=ν,\n",
    "                           M=M,\n",
    "                           key=jax.random.PRNGKey(1)):\n",
    "\n",
    "    s = jnp.full(M, np.log(S0))\n",
    "    h = jnp.full(M, h0)\n",
    "    for t in range(n):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        Z = jax.random.normal(subkey, (2, M))\n",
    "        s = s + μ + jnp.exp(h) * Z[0, :]\n",
    "        h = ρ * h + ν * Z[1, :]\n",
    "    expectation = jnp.mean(jnp.maximum(jnp.exp(s) - K, 0))\n",
    "        \n",
    "    return β**n * expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d4cdf",
   "metadata": {},
   "source": [
    "Let's run it once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ba0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_call_price_jax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf68522",
   "metadata": {},
   "source": [
    "And now let's time it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "compute_call_price_jax().block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d120eaf",
   "metadata": {},
   "source": [
    "```{solution-end}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   25,
   27,
   42,
   45,
   49,
   53,
   57,
   61,
   65,
   67,
   71,
   75,
   77,
   81,
   83,
   94,
   100,
   104,
   108,
   110,
   121,
   123,
   127,
   129,
   135,
   139,
   143,
   146,
   150,
   155,
   159,
   163,
   169,
   173,
   180,
   185,
   189,
   191,
   195,
   197,
   203,
   205,
   209,
   213,
   217,
   219,
   223,
   226,
   230,
   232,
   236,
   240,
   244,
   246,
   250,
   259,
   263,
   267,
   269,
   283,
   287,
   291,
   294,
   298,
   300,
   314,
   316,
   332,
   337,
   339,
   348,
   350,
   355,
   357,
   365,
   367,
   371,
   373,
   377,
   379,
   389,
   394,
   420,
   428,
   432,
   434,
   440,
   444,
   446,
   450,
   454,
   456,
   471,
   474,
   478,
   482,
   484,
   488,
   497,
   519,
   531,
   536,
   542,
   548,
   550,
   554,
   556,
   560,
   562,
   566,
   568,
   572,
   577,
   580,
   584,
   586,
   590,
   597,
   602,
   605,
   609,
   611,
   615,
   617,
   623,
   627,
   629,
   635,
   637,
   641,
   646,
   649,
   655,
   657,
   688,
   716,
   720,
   722,
   726,
   729
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}