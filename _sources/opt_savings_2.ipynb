{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b88b45",
   "metadata": {},
   "source": [
    "# Optimal Savings II: Alternative Algorithms\n",
    "\n",
    "```{include} _admonition/gpu.md\n",
    "```\n",
    "\n",
    "In {doc}`opt_savings_1` we solved a simple version of the household optimal\n",
    "savings problem via value function iteration (VFI) using JAX.\n",
    "\n",
    "In this lecture we tackle exactly the same problem while adding in two\n",
    "alternative algorithms:\n",
    "\n",
    "* optimistic policy iteration (OPI) and\n",
    "* Howard policy iteration (HPI).\n",
    "\n",
    "We will see that both of these algorithms outperform traditional VFI.\n",
    "\n",
    "One reason for this is that the algorithms have good convergence properties.\n",
    "\n",
    "Another is that one of them, HPI, is particularly well suited to pairing with\n",
    "JAX.\n",
    "\n",
    "The reason is that HPI uses a relatively small number of computationally expensive steps,\n",
    "whereas VFI uses a longer sequence of small steps.\n",
    "\n",
    "In other words, VFI is inherently more sequential than HPI, and sequential\n",
    "routines are hard to parallelize.\n",
    "\n",
    "By comparison, HPI is less sequential -- the small number of computationally\n",
    "intensive steps can be effectively parallelized by JAX.\n",
    "\n",
    "This is particularly valuable when the underlying hardware includes a GPU.\n",
    "\n",
    "Details on VFI, HPI and OPI can be found in [this book](https://dp.quantecon.org), for which a PDF is freely available.\n",
    "\n",
    "Here we assume readers have some knowledge of the algorithms and focus on\n",
    "computation.\n",
    "\n",
    "For the details of the savings model, readers can refer to {doc}`opt_savings_1`.\n",
    "\n",
    "In addition to what’s in Anaconda, this lecture will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7253e5",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e8f38",
   "metadata": {},
   "source": [
    "We will use the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd8ddd",
   "metadata": {},
   "source": [
    "Let's check the GPU we are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb1b24",
   "metadata": {},
   "source": [
    "We'll use 64 bit floats to gain extra precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c195673",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c7267",
   "metadata": {},
   "source": [
    "## Model primitives\n",
    "\n",
    "First we define a model that stores parameters and grids.\n",
    "\n",
    "The {ref}`following code <prgm:create-consumption-model>` is repeated from {doc}`opt_savings_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumption_model(R=1.01,                    # Gross interest rate\n",
    "                             β=0.98,                    # Discount factor\n",
    "                             γ=2,                       # CRRA parameter\n",
    "                             w_min=0.01,                # Min wealth\n",
    "                             w_max=5.0,                 # Max wealth\n",
    "                             w_size=150,                # Grid side\n",
    "                             ρ=0.9, ν=0.1, y_size=100): # Income parameters\n",
    "    \"\"\"\n",
    "    A function that takes in parameters and returns parameters and grids \n",
    "    for the optimal savings problem.\n",
    "    \"\"\"\n",
    "    w_grid = jnp.linspace(w_min, w_max, w_size)\n",
    "    mc = qe.tauchen(n=y_size, rho=ρ, sigma=ν)\n",
    "    y_grid, Q = jnp.exp(mc.state_values), jax.device_put(mc.P)\n",
    "    sizes = w_size, y_size\n",
    "    return (β, R, γ), sizes, (w_grid, y_grid, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbf91b",
   "metadata": {},
   "source": [
    "Here's the right hand side of the Bellman equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19751b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(v, params, sizes, arrays):\n",
    "    \"\"\"\n",
    "    A vectorized version of the right-hand side of the Bellman equation\n",
    "    (before maximization), which is a 3D array representing\n",
    "\n",
    "        B(w, y, w′) = u(Rw + y - w′) + β Σ_y′ v(w′, y′) Q(y, y′)\n",
    "\n",
    "    for all (w, y, w′).\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack\n",
    "    β, R, γ = params\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Compute current rewards r(w, y, wp) as array r[i, j, ip]\n",
    "    w  = jnp.reshape(w_grid, (w_size, 1, 1))    # w[i]   ->  w[i, j, ip]\n",
    "    y  = jnp.reshape(y_grid, (1, y_size, 1))    # z[j]   ->  z[i, j, ip]\n",
    "    wp = jnp.reshape(w_grid, (1, 1, w_size))    # wp[ip] -> wp[i, j, ip]\n",
    "    c = R * w + y - wp\n",
    "\n",
    "    # Calculate continuation rewards at all combinations of (w, y, wp)\n",
    "    v = jnp.reshape(v, (1, 1, w_size, y_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
    "    Q = jnp.reshape(Q, (1, y_size, 1, y_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
    "    EV = jnp.sum(v * Q, axis=3)                 # sum over last index jp\n",
    "\n",
    "    # Compute the right-hand side of the Bellman equation\n",
    "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -jnp.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d2add",
   "metadata": {},
   "source": [
    "## Operators\n",
    "\n",
    "We define a function to compute the current rewards $r_\\sigma$ given policy $\\sigma$,\n",
    "which is defined as the vector\n",
    "\n",
    "$$\n",
    "r_\\sigma(w, y) := r(w, y, \\sigma(w, y)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_r_σ(σ, params, sizes, arrays):\n",
    "    \"\"\"\n",
    "    Compute the array r_σ[i, j] = r[i, j, σ[i, j]], which gives current\n",
    "    rewards given policy σ.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ = params\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Compute r_σ[i, j]\n",
    "    w = jnp.reshape(w_grid, (w_size, 1))\n",
    "    y = jnp.reshape(y_grid, (1, y_size))\n",
    "    wp = w_grid[σ]\n",
    "    c = R * w + y - wp\n",
    "    r_σ = c**(1-γ)/(1-γ)\n",
    "\n",
    "    return r_σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebac4ff",
   "metadata": {},
   "source": [
    "Now we define the policy operator $T_\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_σ(v, σ, params, sizes, arrays):\n",
    "    \"The σ-policy operator.\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ = params\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    r_σ = compute_r_σ(σ, params, sizes, arrays)\n",
    "\n",
    "    # Compute the array v[σ[i, j], jp]\n",
    "    yp_idx = jnp.arange(y_size)\n",
    "    yp_idx = jnp.reshape(yp_idx, (1, 1, y_size))\n",
    "    σ = jnp.reshape(σ, (w_size, y_size, 1))\n",
    "    V = v[σ, yp_idx]\n",
    "\n",
    "    # Convert Q[j, jp] to Q[i, j, jp]\n",
    "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
    "\n",
    "    # Calculate the expected sum Σ_jp v[σ[i, j], jp] * Q[i, j, jp]\n",
    "    EV = jnp.sum(V * Q, axis=2)\n",
    "\n",
    "    return r_σ + β * EV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bdac1",
   "metadata": {},
   "source": [
    "and the Bellman operator $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(v, params, sizes, arrays):\n",
    "    \"The Bellman operator.\"\n",
    "    return jnp.max(B(v, params, sizes, arrays), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43439aa",
   "metadata": {},
   "source": [
    "The next function computes a $v$-greedy policy given $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy(v, params, sizes, arrays):\n",
    "    \"Computes a v-greedy policy, returned as a set of indices.\"\n",
    "    return jnp.argmax(B(v, params, sizes, arrays), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de447d78",
   "metadata": {},
   "source": [
    "The function below computes the value $v_\\sigma$ of following policy $\\sigma$.\n",
    "\n",
    "This lifetime value is a function $v_\\sigma$ that satisfies\n",
    "\n",
    "$$\n",
    "v_\\sigma(w, y) = r_\\sigma(w, y) + \\beta \\sum_{y'} v_\\sigma(\\sigma(w, y), y') Q(y, y')\n",
    "$$\n",
    "\n",
    "We wish to solve this equation for $v_\\sigma$.\n",
    "\n",
    "Suppose we define the linear operator $L_\\sigma$ by\n",
    "\n",
    "$$ \n",
    "(L_\\sigma v)(w, y) = v(w, y) - \\beta \\sum_{y'} v(\\sigma(w, y), y') Q(y, y')\n",
    "$$\n",
    "\n",
    "With this notation, the problem is to solve for $v$ via\n",
    "\n",
    "$$\n",
    "(L_{\\sigma} v)(w, y) = r_\\sigma(w, y)\n",
    "$$\n",
    "\n",
    "In vector for this is $L_\\sigma v = r_\\sigma$, which tells us that the function\n",
    "we seek is\n",
    "\n",
    "$$ \n",
    "v_\\sigma = L_\\sigma^{-1} r_\\sigma \n",
    "$$\n",
    "\n",
    "JAX allows us to solve linear systems defined in terms of operators; the first\n",
    "step is to define the function $L_{\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed768adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_σ(v, σ, params, sizes, arrays):\n",
    "    \"\"\"\n",
    "    Here we set up the linear map v -> L_σ v, where \n",
    "\n",
    "        (L_σ v)(w, y) = v(w, y) - β Σ_y′ v(σ(w, y), y′) Q(y, y′)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    β, R, γ = params\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Set up the array v[σ[i, j], jp]\n",
    "    zp_idx = jnp.arange(y_size)\n",
    "    zp_idx = jnp.reshape(zp_idx, (1, 1, y_size))\n",
    "    σ = jnp.reshape(σ, (w_size, y_size, 1))\n",
    "    V = v[σ, zp_idx]\n",
    "\n",
    "    # Expand Q[j, jp] to Q[i, j, jp]\n",
    "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
    "\n",
    "    # Compute and return v[i, j] - β Σ_jp v[σ[i, j], jp] * Q[j, jp]\n",
    "    return v - β * jnp.sum(V * Q, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab48a3f",
   "metadata": {},
   "source": [
    "Now we can define a function to compute $v_{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7dbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(σ, params, sizes, arrays):\n",
    "    \"Get the value v_σ of policy σ by inverting the linear map L_σ.\"\n",
    "\n",
    "    # Unpack\n",
    "    β, R, γ = params\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    r_σ = compute_r_σ(σ, params, sizes, arrays)\n",
    "\n",
    "    # Reduce L_σ to a function in v\n",
    "    partial_L_σ = lambda v: L_σ(v, σ, params, sizes, arrays)\n",
    "\n",
    "    return jax.scipy.sparse.linalg.bicgstab(partial_L_σ, r_σ)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d3263",
   "metadata": {},
   "source": [
    "## JIT compiled versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59afbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = jax.jit(B, static_argnums=(2,))\n",
    "compute_r_σ = jax.jit(compute_r_σ, static_argnums=(2,))\n",
    "T = jax.jit(T, static_argnums=(2,))\n",
    "get_greedy = jax.jit(get_greedy, static_argnums=(2,))\n",
    "get_value = jax.jit(get_value, static_argnums=(2,))\n",
    "T_σ = jax.jit(T_σ, static_argnums=(3,))\n",
    "L_σ = jax.jit(L_σ, static_argnums=(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49eb67",
   "metadata": {},
   "source": [
    "We use successive approximation for VFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_approx_jax(T,                     # Operator (callable)\n",
    "                          x_0,                   # Initial condition                \n",
    "                          tol=1e-6,              # Error tolerance\n",
    "                          max_iter=10_000):      # Max iteration bound\n",
    "    def update(inputs):\n",
    "        k, x, error = inputs\n",
    "        x_new = T(x)\n",
    "        error = jnp.max(jnp.abs(x_new - x))\n",
    "        return k + 1, x_new, error\n",
    "\n",
    "    def condition_function(inputs):\n",
    "        k, x, error = inputs\n",
    "        return jnp.logical_and(error > tol, k < max_iter)\n",
    "\n",
    "    k, x, error = jax.lax.while_loop(condition_function, \n",
    "                                     update, \n",
    "                                     (1, x_0, tol + 1))\n",
    "    return x\n",
    "\n",
    "successive_approx_jax = jax.jit(successive_approx_jax, static_argnums=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785c41d",
   "metadata": {},
   "source": [
    "For OPI we'll add a compiled routine that computes $T_σ^m v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc13b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_policy_operator(σ, v, m, params, sizes, arrays):\n",
    "\n",
    "    def update(i, v):\n",
    "        v = T_σ(v, σ, params, sizes, arrays)\n",
    "        return v\n",
    "    \n",
    "    v = jax.lax.fori_loop(0, m, update, v)\n",
    "    return v\n",
    "\n",
    "iterate_policy_operator = jax.jit(iterate_policy_operator,\n",
    "                                  static_argnums=(4,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca700d14",
   "metadata": {},
   "source": [
    "## Solvers\n",
    "\n",
    "Now we define the solvers, which implement VFI, HPI and OPI.\n",
    "\n",
    "Here's VFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_iteration(model, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Implements value function iteration.\n",
    "    \"\"\"\n",
    "    params, sizes, arrays = model\n",
    "    vz = jnp.zeros(sizes)\n",
    "    _T = lambda v: T(v, params, sizes, arrays)\n",
    "    v_star = successive_approx_jax(_T, vz, tol=tol)\n",
    "    return get_greedy(v_star, params, sizes, arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5321743",
   "metadata": {},
   "source": [
    "For OPI we will use a compiled JAX `lax.while_loop` operation to speed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7dd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opi_loop(params, sizes, arrays, m, tol, max_iter):\n",
    "    \"\"\"\n",
    "    Implements optimistic policy iteration (see dp.quantecon.org) with \n",
    "    step size m.\n",
    "\n",
    "    \"\"\"\n",
    "    v_init = jnp.zeros(sizes)\n",
    "\n",
    "    def condition_function(inputs):\n",
    "        i, v, error = inputs\n",
    "        return jnp.logical_and(error > tol, i < max_iter)\n",
    "\n",
    "    def update(inputs):\n",
    "        i, v, error = inputs\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, params, sizes, arrays)\n",
    "        v = iterate_policy_operator(σ, v, m, params, sizes, arrays)\n",
    "        error = jnp.max(jnp.abs(v - last_v))\n",
    "        i += 1\n",
    "        return i, v, error\n",
    "\n",
    "    num_iter, v, error = jax.lax.while_loop(condition_function,\n",
    "                                            update,\n",
    "                                            (0, v_init, tol + 1))\n",
    "\n",
    "    return get_greedy(v, params, sizes, arrays)\n",
    "\n",
    "opi_loop = jax.jit(opi_loop, static_argnums=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c43a99",
   "metadata": {},
   "source": [
    "Here's a friendly interface to OPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_policy_iteration(model, m=10, tol=1e-5, max_iter=10_000):\n",
    "    params, sizes, arrays = model\n",
    "    σ_star = opi_loop(params, sizes, arrays, m, tol, max_iter)\n",
    "    return σ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef9d9f",
   "metadata": {},
   "source": [
    "Here's HPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb88430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def howard_policy_iteration(model, maxiter=250):\n",
    "    \"\"\"\n",
    "    Implements Howard policy iteration (see dp.quantecon.org)\n",
    "    \"\"\"\n",
    "    params, sizes, arrays = model\n",
    "    σ = jnp.zeros(sizes, dtype=int)\n",
    "    i, error = 0, 1.0\n",
    "    while error > 0 and i < maxiter:\n",
    "        v_σ = get_value(σ, params, sizes, arrays)\n",
    "        σ_new = get_greedy(v_σ, params, sizes, arrays)\n",
    "        error = jnp.max(jnp.abs(σ_new - σ))\n",
    "        σ = σ_new\n",
    "        i = i + 1\n",
    "        print(f\"Concluded loop {i} with error {error}.\")\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d04189",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Create a model for consumption, perform policy iteration, and plot the resulting optimal policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_consumption_model()\n",
    "# Unpack\n",
    "params, sizes, arrays = model\n",
    "β, R, γ = params\n",
    "w_size, y_size = sizes\n",
    "w_grid, y_grid, Q = arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66717be",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Optimal policy function",
      "name": "optimal-policy-function"
     }
    }
   },
   "outputs": [],
   "source": [
    "σ_star = howard_policy_iteration(model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_grid, w_grid, \"k--\", label=\"45\")\n",
    "ax.plot(w_grid, w_grid[σ_star[:, 1]], label=\"$\\\\sigma^*(\\cdot, y_1)$\")\n",
    "ax.plot(w_grid, w_grid[σ_star[:, -1]], label=\"$\\\\sigma^*(\\cdot, y_N)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bd2fc",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Here's a quick test of the timing of each solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89609079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_consumption_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting HPI.\")\n",
    "start_time = time.time()\n",
    "out = howard_policy_iteration(model)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"HPI completed in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting VFI.\")\n",
    "start_time = time.time()\n",
    "out = value_function_iteration(model)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"VFI completed in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3eca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting OPI.\")\n",
    "start_time = time.time()\n",
    "out = optimistic_policy_iteration(model, m=100)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"OPI completed in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(algorithm, model, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = algorithm(model, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{algorithm.__name__} completed in {elapsed_time:.2f} seconds.\")\n",
    "    return result, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_consumption_model()\n",
    "σ_pi, pi_time = run_algorithm(howard_policy_iteration, model)\n",
    "σ_vfi, vfi_time = run_algorithm(value_function_iteration, model, tol=1e-5)\n",
    "\n",
    "m_vals = range(5, 600, 40)\n",
    "opi_times = []\n",
    "for m in m_vals:\n",
    "    σ_opi, opi_time = run_algorithm(optimistic_policy_iteration, \n",
    "                                    model, m=m, tol=1e-5)\n",
    "    opi_times.append(opi_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833f893",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Solver times",
      "name": "howard+value+optimistic-solver-times"
     }
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(m_vals, \n",
    "        jnp.full(len(m_vals), pi_time), \n",
    "        lw=2, label=\"Howard policy iteration\")\n",
    "ax.plot(m_vals, \n",
    "        jnp.full(len(m_vals), vfi_time), \n",
    "        lw=2, label=\"value function iteration\")\n",
    "ax.plot(m_vals, opi_times, \n",
    "        lw=2, label=\"optimistic policy iteration\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlabel(\"$m$\")\n",
    "ax.set_ylabel(\"time\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   56,
   60,
   64,
   71,
   75,
   77,
   81,
   83,
   91,
   108,
   112,
   141,
   152,
   172,
   176,
   200,
   204,
   208,
   212,
   216,
   250,
   274,
   278,
   293,
   297,
   305,
   309,
   330,
   334,
   346,
   355,
   365,
   370,
   399,
   403,
   408,
   413,
   429,
   435,
   444,
   459,
   465,
   469,
   477,
   485,
   493,
   503,
   516
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}