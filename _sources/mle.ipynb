{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fd1397",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "```{include} _admonition/gpu.md\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lecture is the extended JAX implementation of [this section](https://python.quantecon.org/mle.html#mle-with-numerical-methods) of [this lecture](https://python.quantecon.org/mle.html).\n",
    "\n",
    "Please refer that lecture for all background and notation.\n",
    "\n",
    "Here we will exploit the automatic differentiation capabilities of JAX rather than calculating derivatives by hand.\n",
    "\n",
    "We'll require the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from statsmodels.api import Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362f46e",
   "metadata": {},
   "source": [
    "Let's check the GPU we are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87d116",
   "metadata": {},
   "source": [
    "We will use 64 bit floats with JAX in order to increase the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c8441",
   "metadata": {},
   "source": [
    "## MLE with numerical methods (JAX)\n",
    "\n",
    "Many distributions do not have nice, analytical solutions and therefore require\n",
    "numerical methods to solve for parameter estimates.\n",
    "\n",
    "One such numerical method is the Newton-Raphson algorithm.\n",
    "\n",
    "Let's start with a simple example to illustrate the algorithm.\n",
    "\n",
    "### A toy model\n",
    "\n",
    "Our goal is to find the maximum likelihood estimate $\\hat{\\boldsymbol{\\beta}}$.\n",
    "\n",
    "At $\\hat{\\boldsymbol{\\beta}}$, the first derivative of the log-likelihood\n",
    "function will be equal to 0.\n",
    "\n",
    "Let's illustrate this by supposing\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L(\\beta)} = - (\\beta - 10) ^2 - 10\n",
    "$$\n",
    "\n",
    "Define the function `logL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b17a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def logL(β):\n",
    "    return -(β - 10) ** 2 - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2ce8a",
   "metadata": {},
   "source": [
    "To find the value of $\\frac{d \\log \\mathcal{L(\\boldsymbol{\\beta})}}{d \\boldsymbol{\\beta}}$, we can use [jax.grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) which auto-differentiates the given function.\n",
    "\n",
    "We further use [jax.vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) which vectorizes the given function i.e. the function acting upon scalar inputs can now be used with vector inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogL = jax.vmap(jax.grad(logL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "β = jnp.linspace(1, 20)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "\n",
    "ax1.plot(β, logL(β), lw=2)\n",
    "ax2.plot(β, dlogL(β), lw=2)\n",
    "\n",
    "ax1.set_ylabel(r'$log \\mathcal{L(\\beta)}$',\n",
    "               rotation=0,\n",
    "               labelpad=35,\n",
    "               fontsize=15)\n",
    "ax2.set_ylabel(r'$\\frac{dlog \\mathcal{L(\\beta)}}{d \\beta}$ ',\n",
    "               rotation=0,\n",
    "               labelpad=35,\n",
    "               fontsize=19)\n",
    "\n",
    "ax2.set_xlabel(r'$\\beta$', fontsize=15)\n",
    "ax1.grid(), ax2.grid()\n",
    "plt.axhline(c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2a7ae",
   "metadata": {},
   "source": [
    "The plot shows that the maximum likelihood value (the top plot) occurs\n",
    "when $\\frac{d \\log \\mathcal{L(\\boldsymbol{\\beta})}}{d \\boldsymbol{\\beta}} = 0$ (the bottom\n",
    "plot).\n",
    "\n",
    "Therefore, the likelihood is maximized when $\\beta = 10$.\n",
    "\n",
    "We can also ensure that this value is a *maximum* (as opposed to a\n",
    "minimum) by checking that the second derivative (slope of the bottom\n",
    "plot) is negative.\n",
    "\n",
    "The Newton-Raphson algorithm finds a point where the first derivative is\n",
    "0.\n",
    "\n",
    "To use the algorithm, we take an initial guess at the maximum value,\n",
    "$\\beta_0$ (the OLS parameter estimates might be a reasonable\n",
    "guess).\n",
    "\n",
    "Then we use the updating rule involving gradient information to iterate the algorithm until the error is sufficiently small or the algorithm reaches the maximum number of iterations.\n",
    "\n",
    "Please refer to [this section](https://python.quantecon.org/mle.html#mle-with-numerical-methods) for the detailed algorithm.\n",
    "\n",
    "### A Poisson model\n",
    "\n",
    "Let's have a go at implementing the Newton-Raphson algorithm to calculate the maximum likelihood estimations of a Poisson  regression.\n",
    "\n",
    "The Poisson regression has a joint pmf:\n",
    "\n",
    "$$\n",
    "f(y_1, y_2, \\ldots, y_n \\mid \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n; \\boldsymbol{\\beta})\n",
    "    = \\prod_{i=1}^{n} \\frac{\\mu_i^{y_i}}{y_i!} e^{-\\mu_i}\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where}\\ \\mu_i\n",
    "     = \\exp(\\mathbf{x}_i' \\boldsymbol{\\beta})\n",
    "     = \\exp(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik})\n",
    "$$\n",
    "\n",
    "We create a `namedtuple` to store the observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegressionModel = namedtuple('RegressionModel', ['X', 'y'])\n",
    "\n",
    "def create_regression_model(X, y):\n",
    "    n, k = X.shape\n",
    "    # Reshape y as a n_by_1 column vector\n",
    "    y = y.reshape(n, 1)\n",
    "    X, y = jax.device_put((X, y))\n",
    "    return RegressionModel(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d332b1",
   "metadata": {},
   "source": [
    "The log likelihood function of the Poisson regression is\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\max} \\Big(\n",
    "\\sum_{i=1}^{n} y_i \\log{\\mu_i} -\n",
    "\\sum_{i=1}^{n} \\mu_i -\n",
    "\\sum_{i=1}^{n} \\log y! \\Big)\n",
    "$$\n",
    "\n",
    "The full derivation can be found [here](https://python.quantecon.org/mle.html#id2).\n",
    "\n",
    "The log likelihood function involves factorial, but JAX doesn't have a readily available implementation to compute factorial directly.\n",
    "\n",
    "In order to compute the factorial efficiently such that we can JIT it, we use\n",
    "\n",
    "$$\n",
    "    n! = e^{\\log(\\Gamma(n+1))}\n",
    "$$\n",
    "\n",
    "since [jax.lax.lgamma](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.lgamma.html) and [jax.lax.exp](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.exp.html) are available.\n",
    "\n",
    "The following function `jax_factorial` computes the factorial using this idea.\n",
    "\n",
    "Let's define this function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef14df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def _factorial(n):\n",
    "    return jax.lax.exp(jax.lax.lgamma(n + 1.0)).astype(int)\n",
    "\n",
    "jax_factorial = jax.vmap(_factorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ffe28",
   "metadata": {},
   "source": [
    "Now we can define the log likelihood function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def poisson_logL(β, model):\n",
    "    y = model.y\n",
    "    μ = jnp.exp(model.X @ β)\n",
    "    return jnp.sum(model.y * jnp.log(μ) - μ - jnp.log(jax_factorial(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92fc50",
   "metadata": {},
   "source": [
    "To find the gradient of the `poisson_logL`, we again use [jax.grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html).\n",
    "\n",
    "According to [the documentation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev),\n",
    "\n",
    "* `jax.jacfwd` uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while\n",
    "* `jax.jacrev` uses reverse-mode, which is more efficient for “wide” Jacobian matrices.\n",
    "\n",
    "(The documentation also states that when matrices that are near-square, `jax.jacfwd` probably has an edge over `jax.jacrev`.)\n",
    "\n",
    "Therefore, to find the Hessian, we can directly use `jax.jacfwd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d124e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_poisson_logL = jax.grad(poisson_logL)\n",
    "H_poisson_logL = jax.jacfwd(G_poisson_logL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c830748",
   "metadata": {},
   "source": [
    "Our function `newton_raphson` will take a `RegressionModel` object\n",
    "that has an initial guess of the parameter vector $\\boldsymbol{\\beta}_0$.\n",
    "\n",
    "The algorithm will update the parameter vector according to the updating\n",
    "rule, and recalculate the gradient and Hessian matrices at the new\n",
    "parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson(model, β, tol=1e-3, max_iter=100, display=True):\n",
    "\n",
    "    i = 0\n",
    "    error = 100  # Initial error value\n",
    "\n",
    "    # Print header of output\n",
    "    if display:\n",
    "        header = f'{\"Iteration_k\":<13}{\"Log-likelihood\":<16}{\"θ\":<60}'\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "    # While loop runs while any value in error is greater\n",
    "    # than the tolerance until max iterations are reached\n",
    "    while jnp.any(error > tol) and i < max_iter:\n",
    "        H, G = jnp.squeeze(H_poisson_logL(β, model)), G_poisson_logL(β, model)\n",
    "        β_new = β - (jnp.dot(jnp.linalg.inv(H), G))\n",
    "        error = jnp.abs(β_new - β)\n",
    "        β = β_new\n",
    "\n",
    "        if display:\n",
    "            β_list = [f'{t:.3}' for t in list(β.flatten())]\n",
    "            update = f'{i:<13}{poisson_logL(β, model):<16.8}{β_list}'\n",
    "            print(update)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f'Number of iterations: {i}')\n",
    "    print(f'β_hat = {β.flatten()}')\n",
    "\n",
    "    return β"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4bef5",
   "metadata": {},
   "source": [
    "Let's try out our algorithm with a small dataset of 5 observations and 3\n",
    "variables in $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dabdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.array([[1, 2, 5],\n",
    "               [1, 1, 3],\n",
    "               [1, 4, 2],\n",
    "               [1, 5, 2],\n",
    "               [1, 3, 1]])\n",
    "\n",
    "y = jnp.array([1, 0, 1, 1, 0])\n",
    "\n",
    "# Take a guess at initial βs\n",
    "init_β = jnp.array([0.1, 0.1, 0.1]).reshape(X.shape[1], 1)\n",
    "\n",
    "# Create an object with Poisson model values\n",
    "poi = create_regression_model(X, y)\n",
    "\n",
    "# Use newton_raphson to find the MLE\n",
    "β_hat = newton_raphson(poi, init_β, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36979ca",
   "metadata": {},
   "source": [
    "As this was a simple model with few observations, the algorithm achieved\n",
    "convergence in only 7 iterations.\n",
    "\n",
    "The gradient vector should be close to 0 at $\\hat{\\boldsymbol{\\beta}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_poisson_logL(β_hat, poi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb224c74",
   "metadata": {},
   "source": [
    "## MLE with `statsmodels`\n",
    "\n",
    "We’ll use the Poisson regression model in `statsmodels` to verify the results\n",
    "obtained using JAX.\n",
    "\n",
    "`statsmodels` uses the same algorithm as above to find the maximum\n",
    "likelihood estimates.\n",
    "\n",
    "Now, as `statsmodels` accepts only NumPy arrays, we can use the `__array__` method\n",
    "of JAX arrays to convert it to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fea367",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy = X.__array__()\n",
    "y_numpy = y.__array__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc58281",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_poisson = Poisson(y_numpy, X_numpy).fit()\n",
    "print(stats_poisson.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0d099",
   "metadata": {},
   "source": [
    "The benefit of writing our own procedure, relative to statsmodels is that\n",
    "\n",
    "* we can exploit the power of the GPU and\n",
    "* we learn the underlying methodology, which can be extended to complex situations where no existing routines are available.\n",
    "\n",
    "```{exercise-start}\n",
    ":label: newton_mle1\n",
    "```\n",
    "\n",
    "We define a quadratic model for a single explanatory variable by\n",
    "\n",
    "$$\n",
    "    \\log(\\lambda_t) = \\beta_0 + \\beta_1 x_t + \\beta_2 x_{t}^2\n",
    "$$\n",
    "\n",
    "We calculate the mean on the original scale instead of the log scale by exponentiating both sides of the above equation, which gives\n",
    "\n",
    "```{math}\n",
    ":label: lambda_mle\n",
    "    \\lambda_t = \\exp(\\beta_0 + \\beta_1 x_t + \\beta_2 x_{t}^2)\n",
    "```\n",
    "\n",
    "Simulate the values of $x_t$ by sampling from a normal distribution and $\\lambda_t$ by using {eq}`lambda_mle` and the following constants:\n",
    "\n",
    "$$\n",
    "    \\beta_0 = -2.5,\n",
    "    \\quad\n",
    "    \\beta_1 = 0.25,\n",
    "    \\quad\n",
    "    \\beta_2 = 0.5\n",
    "$$\n",
    "\n",
    "Try to obtain the approximate values of $\\beta_0,\\beta_1,\\beta_2$, by simulating a Poisson Regression Model such that\n",
    "\n",
    "$$\n",
    "      y_t \\sim {\\rm Poisson}(\\lambda_t)\n",
    "      \\quad \\text{for all } t.\n",
    "$$\n",
    "\n",
    "Using our `newton_raphson` function on the data set $X = [1, x_t, x_t^{2}]$ and\n",
    "$y$, obtain the maximum likelihood estimates of $\\beta_0,\\beta_1,\\beta_2$.\n",
    "\n",
    "With a sufficient large sample size, you should approximately\n",
    "recover the true values of of these parameters.\n",
    "\n",
    "\n",
    "```{exercise-end}\n",
    "```\n",
    "\n",
    "```{solution-start} newton_mle1\n",
    ":class: dropdown\n",
    "```\n",
    "\n",
    "Let's start by defining \"true\" parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0144ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "β_0 = -2.5\n",
    "β_1 = 0.25\n",
    "β_2 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076cf25",
   "metadata": {},
   "source": [
    "To simulate the model, we sample 500,000 values of $x_t$ from the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 32\n",
    "shape = (500_000, 1)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "x = jax.random.normal(key, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971a864",
   "metadata": {},
   "source": [
    "We compute $\\lambda$ using {eq}`lambda_mle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = jnp.exp(β_0 + β_1 * x + β_2 * x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098dd32",
   "metadata": {},
   "source": [
    "Let's define $y_t$ by sampling from a Poisson distribution with mean as $\\lambda_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d604a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = jax.random.poisson(key, λ, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53bc6f",
   "metadata": {},
   "source": [
    "Now let's try to recover the true parameter values using the Newton-Raphson\n",
    "method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.hstack((jnp.ones(shape), x, x**2))\n",
    "\n",
    "# Take a guess at initial βs\n",
    "init_β = jnp.array([0.1, 0.1, 0.1]).reshape(X.shape[1], 1)\n",
    "\n",
    "# Create an object with Poisson model values\n",
    "poi = create_regression_model(X, y)\n",
    "\n",
    "# Use newton_raphson to find the MLE\n",
    "β_hat = newton_raphson(poi, init_β, tol=1e-5, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93c2b",
   "metadata": {},
   "source": [
    "The maximum likelihood estimates are similar to the true parameter values.\n",
    "\n",
    "```{solution-end}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   33,
   39,
   43,
   45,
   49,
   51,
   77,
   81,
   87,
   91,
   112,
   155,
   164,
   191,
   197,
   202,
   208,
   221,
   224,
   233,
   264,
   269,
   286,
   293,
   295,
   308,
   313,
   316,
   373,
   377,
   381,
   386,
   390,
   392,
   396,
   398,
   404,
   415
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}