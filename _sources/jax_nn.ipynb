{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035eab5e",
   "metadata": {},
   "source": [
    "# Neural Network Regression with JAX \n",
    "\n",
    "```{include} _admonition/gpu.md\n",
    "```\n",
    "\n",
    "## Outline\n",
    "\n",
    "In a {doc}`previous lecture <keras>`, we showed how to implement regression\n",
    "using a neural network via the deep learning library [Keras](https://keras.io/).\n",
    "\n",
    "In this lecture, we solve the same problem directly, using JAX operations rather than relying on the Keras frontend.\n",
    "\n",
    "\n",
    "The objectives are\n",
    "\n",
    "* Understand the nuts and bolts of the exercise better\n",
    "* Explore more features of JAX\n",
    "* Observe how using JAX directly allows us to greatly improve performance.\n",
    "\n",
    "The lecture proceeds in three stages:\n",
    "\n",
    "1. We solve the problem using Keras, to give ourselves a benchmark.  \n",
    "1. We solve the same problem in pure JAX, using pytree operations and gradient descent.  \n",
    "1. We solve the same problem using a combination of JAX and [Optax](https://optax.readthedocs.io/en/latest/index.html), an optimization library built for JAX.  \n",
    "\n",
    "\n",
    "We begin with imports and installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a59de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf3a4e",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install keras optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'jax'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c36bb8",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Without setting the backend to JAX, the imports below might fail.\n",
    "\n",
    "If you have problems running the next cell in Jupyter, try \n",
    "\n",
    "1. quitting\n",
    "2. running `export KERAS_BACKEND=\"jax\"` \n",
    "3. starting Jupyter on the command line from the same terminal.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c88760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7dcd25",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Here we briefly describe the problem and generate synthetic data.\n",
    "\n",
    "\n",
    "### Flow\n",
    "\n",
    "We use the routine from {doc}`keras` to generate data for one-dimensional\n",
    "nonlinear regression.\n",
    "\n",
    "Then we will create a dense (i.e., fully connected) neural network with\n",
    "4 layers, where the input and hidden layers map to $k$-dimensional output space.\n",
    "\n",
    "The inputs and outputs are scalar (for one-dimensional nonlinear regression), so\n",
    "the overall mapping is\n",
    "\n",
    "$$ \\mathbb R \\to \\mathbb R^k \\to \\mathbb R^k \\to \\mathbb R^k \\to \\mathbb R $$\n",
    "\n",
    "Here's a class to store the learning-related constants we’ll use across all implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Config(NamedTuple):\n",
    "    epochs: int = 4000             # Number of passes through the data set\n",
    "    data_size: int = 400           # Sample size\n",
    "    num_layers: int = 4            # Depth of the network\n",
    "    output_dim: int = 10           # Output dimension k of input and hidden layers\n",
    "    learning_rate: float = 0.001   # Learning rate for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ff23f",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Here's the function to generate the data for our regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed66f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "        key: jax.Array,         # JAX random key\n",
    "        config: Config,         # contains configuration data\n",
    "        x_min: float = 0.0,     # Minimum x value\n",
    "        x_max: float = 5.0      # Maximum x value\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data.\n",
    "    \"\"\"\n",
    "    x = jnp.linspace(x_min, x_max, num=config.data_size)\n",
    "    ϵ = 0.2 * jax.random.normal(key, shape=(config.data_size,))\n",
    "    y = x**0.5 + jnp.sin(x) + ϵ\n",
    "    # Return observations as column vectors\n",
    "    x = jnp.reshape(x, (config.data_size, 1))\n",
    "    y = jnp.reshape(y, (config.data_size, 1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823e454",
   "metadata": {},
   "source": [
    "Here's a plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd532284",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "key = jax.random.PRNGKey(1234)\n",
    "x, y = generate_data(key, config)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e0c00",
   "metadata": {},
   "source": [
    "## Training with Keras\n",
    "\n",
    "We build a Keras model that can fit a nonlinear function to the generated data\n",
    "using an ANN.\n",
    "\n",
    "We will use this fit as a benchmark to test our JAX code.\n",
    "\n",
    "Since its role is only a benchmark, we refer readers to the {doc}`previous lecture <keras>` for details on the Keras interface.\n",
    "\n",
    "We start with a function to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(\n",
    "        config: Config,                     # contains configuration data\n",
    "        activation_function: str = 'tanh'   # activation with default\n",
    "    ):\n",
    "    model = Sequential()\n",
    "    # Add layers to the network sequentially, from inputs towards outputs\n",
    "    for i in range(config.num_layers-1):\n",
    "        model.add(\n",
    "           Dense(units=config.output_dim, activation=activation_function)\n",
    "           )\n",
    "    # Add a final layer that maps to a scalar value, for regression.\n",
    "    model.add(Dense(units=1))\n",
    "    # Embed training configurations\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(),  \n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4691d80",
   "metadata": {},
   "source": [
    "Notice that we've set the optimizer to use stochastic gradient descent and a\n",
    "mean square loss.\n",
    "\n",
    "Here is a function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08343591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_model(\n",
    "        model,          # Instance of Keras Sequential model\n",
    "        x,              # Training data, inputs \n",
    "        y,              # Training data, outputs \n",
    "        x_validate,     # Validation data, inputs\n",
    "        y_validate,     # Validation data, outputs\n",
    "        config: Config  # contains configuration data\n",
    "    ):\n",
    "    print(f\"Training NN using Keras.\")\n",
    "    start_time = time()\n",
    "    training_history = model.fit(\n",
    "        x, y,\n",
    "        batch_size=max(x.shape),\n",
    "        verbose=0,\n",
    "        epochs=config.epochs,\n",
    "        validation_data=(x_validate, y_validate)\n",
    "    )\n",
    "    elapsed = time() - start_time\n",
    "    mse = model.evaluate(x_validate, y_validate, verbose=2)\n",
    "    print(f\"Trained in {elapsed:.2f} seconds, validation data MSE = {mse}\")\n",
    "    return model, training_history, elapsed, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919aefc",
   "metadata": {},
   "source": [
    "The next function extracts and visualizes a prediction from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keras_output(model, x, y, x_validate, y_validate):\n",
    "    y_predict = model.predict(x_validate, verbose=2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    ax.plot(x, y_predict, label=\"fitted model\", color='black')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb711b9",
   "metadata": {},
   "source": [
    "Let's run the Keras training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = build_keras_model(config)\n",
    "key = jax.random.PRNGKey(1234)\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "x, y = generate_data(subkey1, config)\n",
    "x_validate, y_validate = generate_data(subkey2, config)\n",
    "model, training_history, keras_runtime, keras_mse = train_keras_model(\n",
    "    model, x, y, x_validate, y_validate, config\n",
    ")\n",
    "plot_keras_output(model, x, y, x_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2c6a4",
   "metadata": {},
   "source": [
    "The fit is good and we note the relatively low final MSE.\n",
    "\n",
    "\n",
    "## Training with JAX\n",
    "\n",
    "For the JAX implementation, we need to construct the network ourselves, as a map\n",
    "from inputs to outputs.\n",
    "\n",
    "We’ll use the same network structure we used for the Keras implementation.\n",
    "\n",
    "### Background and set up\n",
    "\n",
    "The neural network has the form\n",
    "\n",
    "$$\n",
    "f(\\theta, x) \n",
    "    = (A_3 \\circ \\sigma \\circ A_2 \\circ \\sigma \\circ A_1 \\circ \\sigma \\circ A_0)(x)\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ x $ is a scalar input – a point on the horizontal axis in the Keras estimation above,  \n",
    "- $ \\circ $ means composition of maps,  \n",
    "- $ \\sigma $ is the activation function – in our case, $ \\tanh $, and  \n",
    "- $ A_i $ represents the affine map $ A_i x = W_i x + b_i $.  \n",
    "\n",
    "Each matrix $ W_i $ is called a **weight matrix** and each vector $ b_i $ is called a **bias** term.\n",
    "\n",
    "The symbol $ \\theta $ represents the entire collection of parameters:\n",
    "\n",
    "$$\n",
    "\\theta = (W_0, b_0, W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "$$\n",
    "\n",
    "In fact, when we implement the affine map $ A_i x = W_i x + b_i $, we will work\n",
    "with row vectors rather than column vectors, so that\n",
    "\n",
    "- $ x $ and $ b_i $ are stored as row vectors, and  \n",
    "- the mapping is executed by JAX via the expression `x @ W + b`.  \n",
    "\n",
    "Here’s a function to initialize parameters.\n",
    "\n",
    "The parameter “vector” `θ`  will be stored as a list of dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020750e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(\n",
    "        key: jax.Array,     # JAX random key\n",
    "        config: Config      # contains configuration data\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate an initial parameterization for a feed forward neural network.\n",
    "    \"\"\"\n",
    "    k = config.output_dim\n",
    "    shapes = (\n",
    "        (1, k),  # W_0.shape\n",
    "        (k, k),  # W_1.shape\n",
    "        (k, k),  # W_2.shape\n",
    "        (k, 1)   # W_3.shape\n",
    "    )\n",
    "    # A function to generate weight matrices using JAX random\n",
    "    def w_init(key, m, n):\n",
    "        return jax.random.normal(key, shape=(m, n)) * jnp.sqrt(2 / m)\n",
    "    # Build list of dicts, each containing a (weight, bias) pair\n",
    "    θ = []\n",
    "    for w_shape in shapes:\n",
    "        m, n = w_shape\n",
    "        key, subkey = jax.random.split(key)\n",
    "        layer_params = dict(W=w_init(subkey, m, n), b=jnp.ones((1, n)))\n",
    "        θ.append(layer_params)\n",
    "    return θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5e80d",
   "metadata": {},
   "source": [
    "Wait, you say!\n",
    "\n",
    "Shouldn’t we concatenate the elements of $ \\theta $ into some kind of big array, so that we can do autodiff with respect to this array?\n",
    "\n",
    "Actually we don’t need to --- we use the JAX PyTree approach discussed below.\n",
    "\n",
    "\n",
    "### Coding the network\n",
    "\n",
    "Here’s our implementation of the ANN $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfacfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(\n",
    "        θ: list,                        # Network parameters (pytree)\n",
    "        x: jnp.ndarray,                 # Input data (row vector)\n",
    "        σ: callable = jnp.tanh          # Activation function\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Perform a forward pass over the network to evaluate f(θ, x).\n",
    "    \"\"\"\n",
    "    *hidden, last = θ\n",
    "    for layer in hidden:\n",
    "        W, b = layer['W'], layer['b']\n",
    "        x = σ(x @ W + b)\n",
    "    W, b = last['W'], last['b']\n",
    "    x = x @ W + b\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d178f8a",
   "metadata": {},
   "source": [
    "The function $ f $ is appropriately vectorized, so that we can pass in the entire\n",
    "set of input observations as `x` and return the predicted vector of outputs `y_hat = f(θ, x)`\n",
    "corresponding  to each data point.\n",
    "\n",
    "The loss function is mean squared error, the same as the Keras case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13281e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(\n",
    "        θ: list,            # Network parameters (pytree)\n",
    "        x: jnp.ndarray,     # Input data\n",
    "        y: jnp.ndarray      # Target data\n",
    "    ):\n",
    "    return jnp.mean((f(θ, x) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d4c51",
   "metadata": {},
   "source": [
    "We’ll use its gradient to do stochastic gradient descent.\n",
    "\n",
    "(Technically, we will be doing gradient descent, rather than stochastic\n",
    "gradient descent, since will not randomize over sample points when we\n",
    "evaluate the gradient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_gradient = jax.jit(jax.grad(loss_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4827c7",
   "metadata": {},
   "source": [
    "The gradient of `loss_fn` is with respect to the first argument `θ`.\n",
    "\n",
    "The code above seems kind of magical, since we are differentiating with respect\n",
    "to a parameter “vector” stored as a list of dictionaries containing arrays.\n",
    "\n",
    "How can we differentiate with respect to such a complex object?\n",
    "\n",
    "The answer is that the list of dictionaries is treated internally as a\n",
    "[pytree](https://docs.jax.dev/en/latest/pytrees.html).\n",
    "\n",
    "The JAX function `grad` understands how to\n",
    "\n",
    "1. extract the individual arrays (the \"leaves\" of the tree),\n",
    "1. compute derivatives with respect to each one, and\n",
    "1. pack the resulting derivatives into a pytree with the same structure as the parameter vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493951f",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Using the above code, we can now write our rule for updating the parameters via gradient descent, which is the\n",
    "algorithm we covered in our [lecture on autodiff](https://jax.quantecon.org/autodiff.html).\n",
    "\n",
    "In this case, to keep things as simple as possible, we’ll use a fixed learning rate for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_parameters(\n",
    "        θ: list,            # Current parameters (pytree)\n",
    "        x: jnp.ndarray,     # Input data\n",
    "        y: jnp.ndarray,     # Target data\n",
    "        config: Config      # contains configuration data\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the parameter pytree using gradient descent.\n",
    "\n",
    "    \"\"\"\n",
    "    λ = config.learning_rate\n",
    "    # Specify the update rule\n",
    "    def gradient_descent_step(p, g):\n",
    "        \"\"\"\n",
    "        A rule for updating parameter vector p given gradient vector g.\n",
    "        It will be applied to each leaf of the pytree of parameters.\n",
    "        \"\"\"\n",
    "        return p - λ * g\n",
    "    gradient = loss_gradient(θ, x, y)\n",
    "    # Use tree.map to apply the update rule to the parameter vectors\n",
    "    θ_new = jax.tree.map(gradient_descent_step, θ, gradient)\n",
    "    return θ_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a966b6c",
   "metadata": {},
   "source": [
    "Here `jax.tree.map` understands `θ` and `gradient` as pytrees of the\n",
    "same structure and executes `p - λ * g` on the corresponding leaves of the pair\n",
    "of trees.\n",
    "\n",
    "Each weight matrix and bias vector is updated by gradient\n",
    "descent, exactly as required.\n",
    "\n",
    "Here’s code that puts this all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294cb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_jax_model(\n",
    "        θ: list,                    # Initial parameters (pytree)\n",
    "        x: jnp.ndarray,             # Training input data\n",
    "        y: jnp.ndarray,             # Training target data\n",
    "        x_validate: jnp.ndarray,    # Validation input data\n",
    "        y_validate: jnp.ndarray,    # Validation target data\n",
    "        config: Config              # contains configuration data\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train model using gradient descent.\n",
    "\n",
    "    \"\"\"\n",
    "    def update(θ, _):\n",
    "        train_loss = loss_fn(θ, x, y)\n",
    "        val_loss = loss_fn(θ, x_validate, y_validate)\n",
    "        θ_new = update_parameters(θ, x, y, config)\n",
    "        accumulate = train_loss, val_loss\n",
    "        return θ_new, accumulate\n",
    "\n",
    "    θ_final, (training_losses, validation_losses) = jax.lax.scan(\n",
    "        update, θ, None, length=config.epochs\n",
    "    )\n",
    "    return θ_final, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e3e7c",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Let’s run our code and see how it goes.\n",
    "\n",
    "We'll reuse the data we generated for the Keras experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "param_key = jax.random.PRNGKey(1234)\n",
    "θ = initialize_params(param_key, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d32555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup run to trigger JIT compilation\n",
    "train_jax_model(θ, x, y, x_validate, y_validate, config)\n",
    "\n",
    "# Reset and time the actual run\n",
    "θ = initialize_params(param_key, config)\n",
    "start_time = time()\n",
    "θ, training_loss, validation_loss = train_jax_model(\n",
    "    θ, x, y, x_validate, y_validate, config\n",
    ")\n",
    "θ[0]['W'].block_until_ready()  # Ensure computation completes\n",
    "jax_runtime = time() - start_time\n",
    "\n",
    "jax_mse = loss_fn(θ, x_validate, y_validate)\n",
    "print(f\"Trained model with JAX in {jax_runtime:.2f} seconds.\")\n",
    "print(f\"Final MSE on validation data = {jax_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046300c6",
   "metadata": {},
   "source": [
    "Despite the simplicity of our implementation, we actually perform slightly better than Keras.\n",
    "\n",
    "This figure shows MSE across iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe099213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(validation_loss)), validation_loss, label='validation loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21750f95",
   "metadata": {},
   "source": [
    "Here’s a visualization of the quality of our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.flatten(), f(θ, x).flatten(), \n",
    "        label=\"fitted model\", color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb702be",
   "metadata": {},
   "source": [
    "## JAX plus Optax\n",
    "\n",
    "Our hand-coded optimization routine above was quite effective, but in practice\n",
    "we might wish to use an optimization library written for JAX.\n",
    "\n",
    "One such library is [Optax](https://optax.readthedocs.io/en/latest/).\n",
    "\n",
    "### Optax with SGD\n",
    "\n",
    "Here’s a training routine using Optax’s stochastic gradient descent solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_jax_optax(\n",
    "        θ: list,                    # Initial parameters (pytree)\n",
    "        x: jnp.ndarray,             # Training input data\n",
    "        y: jnp.ndarray,             # Training target data\n",
    "        epochs: int = 4000,         # Number of training epochs\n",
    "        learning_rate: float = 0.001  # Learning rate for optimizer\n",
    "    ):\n",
    "    \" Train model using Optax SGD optimizer. \"\n",
    "    solver = optax.sgd(learning_rate)\n",
    "    opt_state = solver.init(θ)\n",
    "\n",
    "    def update(_, loop_state):\n",
    "        θ, opt_state = loop_state\n",
    "        grad = loss_gradient(θ, x, y)\n",
    "        updates, new_opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ_new = optax.apply_updates(θ, updates)\n",
    "        new_loop_state = θ_new, new_opt_state\n",
    "        return new_loop_state\n",
    "\n",
    "    initial_loop_state = θ, opt_state\n",
    "    final_loop_state = jax.lax.fori_loop(0, epochs, update, initial_loop_state)\n",
    "    θ_final, _ = final_loop_state\n",
    "    return θ_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2bec4",
   "metadata": {},
   "source": [
    "Let’s try running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99178e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameter vector\n",
    "θ = initialize_params(param_key, config)\n",
    "\n",
    "# Warmup run to trigger JIT compilation\n",
    "train_jax_optax(θ, x, y)\n",
    "\n",
    "# Reset and time the actual run\n",
    "θ = initialize_params(param_key, config)\n",
    "start_time = time()\n",
    "θ = train_jax_optax(θ, x, y)\n",
    "θ[0]['W'].block_until_ready()  # Ensure computation completes\n",
    "optax_sgd_runtime = time() - start_time\n",
    "\n",
    "optax_sgd_mse = loss_fn(θ, x_validate, y_validate)\n",
    "print(f\"Trained model with JAX and Optax SGD in {optax_sgd_runtime:.2f} seconds.\")\n",
    "print(f\"Final MSE on validation data = {optax_sgd_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a151012",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.flatten(), f(θ, x).flatten(), \n",
    "        label=\"fitted model\", color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4da690",
   "metadata": {},
   "source": [
    "### Optax with ADAM\n",
    "\n",
    "We can also consider using a slightly more sophisticated gradient-based method,\n",
    "such as [ADAM](https://arxiv.org/pdf/1412.6980).\n",
    "\n",
    "You will notice that the syntax for using this alternative optimizer is very\n",
    "similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03237a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_jax_optax_adam(\n",
    "        θ: list,                    # Initial parameters (pytree)\n",
    "        x: jnp.ndarray,             # Training input data\n",
    "        y: jnp.ndarray,             # Training target data\n",
    "        epochs: int = 4000,         # Number of training epochs\n",
    "        learning_rate: float = 0.001  # Learning rate for optimizer\n",
    "    ):\n",
    "    \" Train model using Optax ADAM optimizer. \"\n",
    "\n",
    "    solver = optax.adam(learning_rate)\n",
    "    opt_state = solver.init(θ)\n",
    "\n",
    "    def update(_, loop_state):\n",
    "        θ, opt_state = loop_state\n",
    "        grad = loss_gradient(θ, x, y)\n",
    "        updates, new_opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ_new = optax.apply_updates(θ, updates)\n",
    "        return (θ_new, new_opt_state)\n",
    "\n",
    "    initial_loop_state = θ, opt_state\n",
    "    θ_final, _ = jax.lax.fori_loop(0, epochs, update, initial_loop_state)\n",
    "    return θ_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f94be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameter vector\n",
    "θ = initialize_params(param_key, config)\n",
    "\n",
    "# Warmup run to trigger JIT compilation\n",
    "train_jax_optax_adam(θ, x, y)\n",
    "\n",
    "# Reset and time the actual run\n",
    "θ = initialize_params(param_key, config)\n",
    "start_time = time()\n",
    "θ = train_jax_optax_adam(θ, x, y)\n",
    "θ[0]['W'].block_until_ready()  # Ensure computation completes\n",
    "optax_adam_runtime = time() - start_time\n",
    "\n",
    "optax_adam_mse = loss_fn(θ, x_validate, y_validate)\n",
    "print(f\"Trained model with JAX and Optax ADAM in {optax_adam_runtime:.2f} seconds.\")\n",
    "print(f\"Final MSE on validation data = {optax_adam_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08749760",
   "metadata": {},
   "source": [
    "Here's a visualization of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.flatten(), f(θ, x).flatten(),\n",
    "        label=\"fitted model\", color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04481f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we compare the performance of the four different training approaches we explored in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff426c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "results = {\n",
    "    'Method': [\n",
    "        'Keras',\n",
    "        'Pure JAX (hand-coded GD)',\n",
    "        'JAX + Optax SGD',\n",
    "        'JAX + Optax ADAM'\n",
    "    ],\n",
    "    'Runtime (s)': [\n",
    "        keras_runtime,\n",
    "        jax_runtime,\n",
    "        optax_sgd_runtime,\n",
    "        optax_adam_runtime\n",
    "    ],\n",
    "    'Validation MSE': [\n",
    "        keras_mse,\n",
    "        jax_mse,\n",
    "        optax_sgd_mse,\n",
    "        optax_adam_mse\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.style.format({'Runtime (s)': '{:.2f}', 'Validation MSE': '{:.4f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bade4c",
   "metadata": {},
   "source": [
    "All methods achieve similar validation MSE values (around 0.043-0.045).\n",
    "\n",
    "At the time of writing, the MSEs from plain vanilla Optax and our own hand-coded SGD routine are identical.\n",
    "\n",
    "The ADAM optimizer achieves slightly better MSE by using adaptive learning rates.\n",
    "\n",
    "Still, our hand-coded algorithm does very well compared to this high-quality optimizer.\n",
    "\n",
    "Note also that the pure JAX implementations are significantly faster than Keras.\n",
    "\n",
    "This is because JAX can JIT-compile the entire training loop.\n",
    "\n",
    "Not surprisingly, Keras has more overhead from its abstraction layers.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "```{exercise}\n",
    ":label: jax_nn_ex1\n",
    "\n",
    "Try to reduce the MSE on the validation data without significantly increasing the computational load.\n",
    "\n",
    "You should hold constant both the number of epochs and the total number of parameters in the network.\n",
    "\n",
    "Currently, the network has 4 layers with output dimension $k=10$, giving a total of:\n",
    "- Layer 0: $1 \\times 10 + 10 = 20$ parameters (weights + biases)\n",
    "- Layer 1: $10 \\times 10 + 10 = 110$ parameters\n",
    "- Layer 2: $10 \\times 10 + 10 = 110$ parameters\n",
    "- Layer 3: $10 \\times 1 + 1 = 11$ parameters\n",
    "- Total: $251$ parameters\n",
    "\n",
    "You can experiment with:\n",
    "- Changing the network architecture \n",
    "- Trying different activation functions (e.g., `jax.nn.relu`, `jax.nn.gelu`, `jax.nn.sigmoid`, `jax.nn.elu`)\n",
    "- Modifying the optimizer (e.g., different learning rates, learning rate schedules, momentum, other Optax optimizers)\n",
    "- Experimenting with different weight initialization strategies\n",
    "\n",
    "\n",
    "Which combination gives you the lowest validation MSE?\n",
    "```\n",
    "\n",
    "\n",
    "```{solution-start} jax_nn_ex1\n",
    ":class: dropdown\n",
    "```\n",
    "\n",
    "Let's implement and test several strategies. \n",
    "\n",
    "**Strategy 1: Deeper Network Architecture**\n",
    "\n",
    "Let's try a deeper network with 6 layers instead of 4, keeping total parameters ≤ 251:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Deeper network (6 layers with k=6)\n",
    "# Layer sizes: 1→6→6→6→6→6→1\n",
    "# Parameters: (1×6+6) + 4×(6×6+6) + (6×1+1) = 12 + 4×42 + 7 = 187 < 251\n",
    "θ = initialize_params(param_key, config)\n",
    "\n",
    "def initialize_deep_params(key, k=6, num_hidden=5):\n",
    "    \" Initialize parameters for deeper network with k=6. \"\n",
    "    shapes = [(1, k)] + [(k, k)] * (num_hidden - 1) + [(k, 1)]\n",
    "\n",
    "    def w_init(key, m, n):\n",
    "        return jax.random.normal(key, shape=(m, n)) * jnp.sqrt(2 / m)\n",
    "\n",
    "    θ = []\n",
    "    for w_shape in shapes:\n",
    "        m, n = w_shape\n",
    "        key, subkey = jax.random.split(key)\n",
    "        layer_params = dict(W=w_init(subkey, m, n), b=jnp.ones((1, n)))\n",
    "        θ.append(layer_params)\n",
    "    return θ\n",
    "\n",
    "θ_deep = initialize_deep_params(param_key)\n",
    "\n",
    "# Warmup\n",
    "train_jax_optax_adam(θ_deep, x, y)\n",
    "\n",
    "# Actual run\n",
    "θ_deep = initialize_deep_params(param_key)\n",
    "start_time = time()\n",
    "θ_deep = train_jax_optax_adam(θ_deep, x, y)\n",
    "θ_deep[0]['W'].block_until_ready()\n",
    "deep_runtime = time() - start_time\n",
    "\n",
    "deep_mse = loss_fn(θ_deep, x_validate, y_validate)\n",
    "print(f\"Strategy 1 - Deeper network (6 layers, k=6)\")\n",
    "print(f\"  Total parameters: 187\")\n",
    "print(f\"  Runtime: {deep_runtime:.2f}s\")\n",
    "print(f\"  Validation MSE: {deep_mse:.6f}\")\n",
    "print(f\"  Improvement over ADAM: {optax_adam_mse - deep_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3af623",
   "metadata": {},
   "source": [
    "**Strategy 2: Deeper Network + Learning Rate Schedule**\n",
    "\n",
    "Since the deeper network performed best, let's combine it with the learning rate schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Deeper network + LR schedule\n",
    "θ_deep = initialize_deep_params(param_key)\n",
    "\n",
    "def train_deep_with_schedule(\n",
    "        θ: list,\n",
    "        x: jnp.ndarray,\n",
    "        y: jnp.ndarray,\n",
    "        epochs: int = 4000\n",
    "    ):\n",
    "    \" Train deeper network with learning rate schedule. \"\n",
    "\n",
    "    schedule = optax.exponential_decay(\n",
    "        init_value=0.003,\n",
    "        transition_steps=1000,\n",
    "        decay_rate=0.5\n",
    "    )\n",
    "\n",
    "    solver = optax.adam(schedule)\n",
    "    opt_state = solver.init(θ)\n",
    "\n",
    "    def update(_, loop_state):\n",
    "        θ, opt_state = loop_state\n",
    "        grad = loss_gradient(θ, x, y)\n",
    "        updates, new_opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ_new = optax.apply_updates(θ, updates)\n",
    "        return (θ_new, new_opt_state)\n",
    "\n",
    "    initial_loop_state = θ, opt_state\n",
    "    θ_final, _ = jax.lax.fori_loop(0, epochs, update, initial_loop_state)\n",
    "    return θ_final\n",
    "\n",
    "# Warmup\n",
    "train_deep_with_schedule(θ_deep, x, y)\n",
    "\n",
    "# Actual run\n",
    "θ_deep = initialize_deep_params(param_key)\n",
    "start_time = time()\n",
    "θ_deep_schedule = train_deep_with_schedule(θ_deep, x, y)\n",
    "θ_deep_schedule[0]['W'].block_until_ready()\n",
    "deep_schedule_runtime = time() - start_time\n",
    "\n",
    "deep_schedule_mse = loss_fn(θ_deep_schedule, x_validate, y_validate)\n",
    "print(f\"Strategy 2 - Deeper network + LR schedule\")\n",
    "print(f\"  Runtime: {deep_schedule_runtime:.2f}s\")\n",
    "print(f\"  Validation MSE: {deep_schedule_mse:.6f}\")\n",
    "print(f\"  Improvement over ADAM: {optax_adam_mse - deep_schedule_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f2de3",
   "metadata": {},
   "source": [
    "**Strategy 3: ELU Activation**\n",
    "\n",
    "Let's try ELU (Exponential Linear Unit), a modern activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: ELU activation\n",
    "θ = initialize_params(param_key, config)\n",
    "\n",
    "def train_with_elu(\n",
    "        θ: list,\n",
    "        x: jnp.ndarray,\n",
    "        y: jnp.ndarray,\n",
    "        epochs: int = 4000,\n",
    "        learning_rate: float = 0.001\n",
    "    ):\n",
    "    \" Train model using ELU activation and Optax ADAM optimizer. \"\n",
    "\n",
    "    # Modified forward pass with ELU\n",
    "    @jax.jit\n",
    "    def f_elu(θ, x):\n",
    "        *hidden, last = θ\n",
    "        for layer in hidden:\n",
    "            W, b = layer['W'], layer['b']\n",
    "            x = jax.nn.elu(x @ W + b)\n",
    "        W, b = last['W'], last['b']\n",
    "        x = x @ W + b\n",
    "        return x\n",
    "\n",
    "    # Modified loss function\n",
    "    @jax.jit\n",
    "    def loss_fn_elu(θ, x, y):\n",
    "        return jnp.mean((f_elu(θ, x) - y)**2)\n",
    "\n",
    "    loss_gradient_elu = jax.jit(jax.grad(loss_fn_elu))\n",
    "\n",
    "    solver = optax.adam(learning_rate)\n",
    "    opt_state = solver.init(θ)\n",
    "\n",
    "    def update(_, loop_state):\n",
    "        θ, opt_state = loop_state\n",
    "        grad = loss_gradient_elu(θ, x, y)\n",
    "        updates, new_opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ_new = optax.apply_updates(θ, updates)\n",
    "        return (θ_new, new_opt_state)\n",
    "\n",
    "    initial_loop_state = θ, opt_state\n",
    "    θ_final, _ = jax.lax.fori_loop(0, epochs, update, initial_loop_state)\n",
    "    return θ_final, f_elu, loss_fn_elu\n",
    "\n",
    "# Warmup run\n",
    "θ_elu, f_elu, loss_fn_elu = train_with_elu(θ, x, y)\n",
    "\n",
    "# Actual run\n",
    "θ = initialize_params(param_key, config)\n",
    "start_time = time()\n",
    "θ_elu, f_elu, loss_fn_elu = train_with_elu(θ, x, y)\n",
    "θ_elu[0]['W'].block_until_ready()\n",
    "elu_runtime = time() - start_time\n",
    "\n",
    "elu_mse = loss_fn_elu(θ_elu, x_validate, y_validate)\n",
    "print(f\"Strategy 3 - ELU activation\")\n",
    "print(f\"  Runtime: {elu_runtime:.2f}s\")\n",
    "print(f\"  Validation MSE: {elu_mse:.6f}\")\n",
    "print(f\"  Improvement over ADAM: {optax_adam_mse - elu_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd57475",
   "metadata": {},
   "source": [
    "**Strategy 4: SELU Activation**\n",
    "\n",
    "Let's try SELU (Scaled Exponential Linear Unit), another modern activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65961a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: SELU activation\n",
    "θ = initialize_params(param_key, config)\n",
    "\n",
    "def train_with_selu(\n",
    "        θ: list,\n",
    "        x: jnp.ndarray,\n",
    "        y: jnp.ndarray,\n",
    "        epochs: int = 4000,\n",
    "        learning_rate: float = 0.001\n",
    "    ):\n",
    "    \" Train model using SELU activation and Optax ADAM optimizer. \"\n",
    "\n",
    "    # Modified forward pass with SELU\n",
    "    @jax.jit\n",
    "    def f_selu(θ, x):\n",
    "        *hidden, last = θ\n",
    "        for layer in hidden:\n",
    "            W, b = layer['W'], layer['b']\n",
    "            x = jax.nn.selu(x @ W + b)\n",
    "        W, b = last['W'], last['b']\n",
    "        x = x @ W + b\n",
    "        return x\n",
    "\n",
    "    # Modified loss function\n",
    "    @jax.jit\n",
    "    def loss_fn_selu(θ, x, y):\n",
    "        return jnp.mean((f_selu(θ, x) - y)**2)\n",
    "\n",
    "    loss_gradient_selu = jax.jit(jax.grad(loss_fn_selu))\n",
    "\n",
    "    solver = optax.adam(learning_rate)\n",
    "    opt_state = solver.init(θ)\n",
    "\n",
    "    def update(_, loop_state):\n",
    "        θ, opt_state = loop_state\n",
    "        grad = loss_gradient_selu(θ, x, y)\n",
    "        updates, new_opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ_new = optax.apply_updates(θ, updates)\n",
    "        return (θ_new, new_opt_state)\n",
    "\n",
    "    initial_loop_state = θ, opt_state\n",
    "    θ_final, _ = jax.lax.fori_loop(0, epochs, update, initial_loop_state)\n",
    "    return θ_final, f_selu, loss_fn_selu\n",
    "\n",
    "# Warmup run\n",
    "θ_selu, f_selu, loss_fn_selu = train_with_selu(θ, x, y)\n",
    "\n",
    "# Actual run\n",
    "θ = initialize_params(param_key, config)\n",
    "start_time = time()\n",
    "θ_selu, f_selu, loss_fn_selu = train_with_selu(θ, x, y)\n",
    "θ_selu[0]['W'].block_until_ready()\n",
    "selu_runtime = time() - start_time\n",
    "\n",
    "selu_mse = loss_fn_selu(θ_selu, x_validate, y_validate)\n",
    "print(f\"Strategy 4 - SELU activation\")\n",
    "print(f\"  Runtime: {selu_runtime:.2f}s\")\n",
    "print(f\"  Validation MSE: {selu_mse:.6f}\")\n",
    "print(f\"  Improvement over ADAM: {optax_adam_mse - selu_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b9baa",
   "metadata": {},
   "source": [
    "**Results Summary**\n",
    "\n",
    "Let's compare all strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607ab2f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Summary of all strategies\n",
    "strategies_results = {\n",
    "    'Strategy': [\n",
    "        'Baseline (ADAM + tanh)',\n",
    "        '1. Deeper network (6 layers)',\n",
    "        '2. Deeper network + LR schedule',\n",
    "        '3. ELU activation',\n",
    "        '4. SELU activation'\n",
    "    ],\n",
    "    'Runtime (s)': [\n",
    "        optax_adam_runtime,\n",
    "        deep_runtime,\n",
    "        deep_schedule_runtime,\n",
    "        elu_runtime,\n",
    "        selu_runtime\n",
    "    ],\n",
    "    'Validation MSE': [\n",
    "        optax_adam_mse,\n",
    "        deep_mse,\n",
    "        deep_schedule_mse,\n",
    "        elu_mse,\n",
    "        selu_mse\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        0.0,\n",
    "        float(optax_adam_mse - deep_mse),\n",
    "        float(optax_adam_mse - deep_schedule_mse),\n",
    "        float(optax_adam_mse - elu_mse),\n",
    "        float(optax_adam_mse - selu_mse)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_strategies = pd.DataFrame(strategies_results)\n",
    "df_strategies.style.format({\n",
    "    'Runtime (s)': '{:.2f}',\n",
    "    'Validation MSE': '{:.6f}',\n",
    "    'Improvement': '{:.6f}'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e3a60",
   "metadata": {},
   "source": [
    "The experimental results reveal several lessons:\n",
    "\n",
    "1. Architecture matters: A deeper, narrower network outperformed the\n",
    "   baseline network, despite using fewer parameters (187 vs 251).\n",
    "\n",
    "2. Combining strategies: Combining the deeper architecture with a learning\n",
    "   rate schedule yielded the best results, showing that synergistic improvements\n",
    "   are possible.\n",
    "\n",
    "\n",
    "\n",
    "```{solution-end}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.18.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   42,
   51,
   57,
   59,
   71,
   76,
   98,
   107,
   113,
   130,
   134,
   143,
   156,
   175,
   182,
   204,
   208,
   217,
   221,
   232,
   278,
   304,
   317,
   334,
   342,
   350,
   358,
   360,
   378,
   387,
   411,
   422,
   446,
   454,
   460,
   476,
   482,
   487,
   491,
   499,
   512,
   536,
   540,
   559,
   567,
   577,
   603,
   620,
   624,
   632,
   638,
   667,
   722,
   761,
   767,
   814,
   820,
   880,
   886,
   946,
   952,
   993
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}