{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230e057f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\argmin}{arg\\,min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f825409",
   "metadata": {},
   "source": [
    "# Policy Gradient-Based Optimal Savings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5813ae",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with access to a GPU.\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/) has a free tier with GPUs\n",
    "that you can access as follows:\n",
    "\n",
    "1. Click on the “play” icon top right  \n",
    "1. Select Colab  \n",
    "1. Set the runtime environment to include a GPU  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa231436",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we solve infinite horizon optimal savings problems using deep\n",
    "learning and policy gradient ascent with JAX.\n",
    "\n",
    "Each policy is represented as a fully connected feed forward neural network.\n",
    "\n",
    "We begin with a cake eating problem with a known analytical solution.\n",
    "\n",
    "Then we shift to an income fluctuation problem where we can compute an optimal\n",
    "policy easily with the endogenous grid method (EGM).\n",
    "\n",
    "We do this first and then try to learn the same policy with deep learning.\n",
    "\n",
    "The technique we will use is called [policy gradient\n",
    "ascent](https://en.wikipedia.org/wiki/Policy_gradient_method).\n",
    "\n",
    "This method is popular in the machine learning community for solving\n",
    "high-dimensional dynamic programming problems.\n",
    "\n",
    "Since the income fluctuation problem is low-dimensional, the policy gradient\n",
    "method will not be superior to EGM.\n",
    "\n",
    "However, by working through this lecture, we can learn the basic principles of\n",
    "policy gradient methods and see them work in practice.\n",
    "\n",
    "We’ll use the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4662c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6aa7ff",
   "metadata": {},
   "source": [
    "We’ll use the following imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03863e7f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d0ed5",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Let’s describe the income fluctuation problem and the ideas behind policy\n",
    "gradient ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c6462",
   "metadata": {},
   "source": [
    "### Household problem\n",
    "\n",
    "A household chooses a consumption plan $ \\{c_t\\}_{t \\geq 0} $ to maximize\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\, \\sum_{t=0}^{\\infty} \\beta^t u(c_t)\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "a_{t+1} = R (a_t - c_t) + Y_{t+1},\n",
    "    \\quad c_t \\geq 0, \\quad a_t \\geq 0, \\quad t = 0, 1, \\ldots\n",
    "$$\n",
    "\n",
    "Here $ Y_t $ is labor income, which is IID and normally distributed:\n",
    "\n",
    "$$\n",
    "Z_t \\sim N(m, v), \\quad Y_t = \\exp(Z_t)\n",
    "$$\n",
    "\n",
    "We assume:\n",
    "\n",
    "1. $ \\beta R < 1 $  \n",
    "1. $ u $ is CRRA with parameter $ \\gamma $  \n",
    "\n",
    "\n",
    "We will be interested in the value of alternative policy functions for this\n",
    "household.\n",
    "\n",
    "Since the shocks are IID, and hence offer no predictive content for future shocks,\n",
    "optimal policies will depend only on current assets.\n",
    "\n",
    "The next section discusses policies and their values in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7326aa2",
   "metadata": {},
   "source": [
    "### Lifetime Value and Optimization\n",
    "\n",
    "A **policy** is a function $ \\sigma $ from $ \\mathbb{R}_+ $ to itself,\n",
    "where $ \\sigma(a) $ is understood as the amount consumed under policy $ \\sigma $\n",
    "given current state $ a $.\n",
    "\n",
    "A **feasible policy** is a\n",
    "([measurable](https://en.wikipedia.org/wiki/Measurable_function)) policy\n",
    "satisfying $ 0 \\leq \\sigma(a) \\leq a $ for all $ a $ (no borrowing).\n",
    "\n",
    "We let $ \\Sigma $ denote the set of all feasible policies.\n",
    "\n",
    "We let $ v_\\sigma(a) $ be the lifetime value of following policy\n",
    "$ \\sigma $, given initial assets $ a $.\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\n",
    "v_\\sigma(a) = \\mathbb{E} \\sum_{t \\geq 0} \\beta^t u(c_t)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ c_t = \\sigma(a_t) $  \n",
    "- $ a_0 = a $  \n",
    "- $ a_{t+1} = R (a_t - \\sigma(a_t)) + Y_{t+1} $ for $ t = 0, 1,\\ldots $  \n",
    "\n",
    "\n",
    "A policy $ \\sigma $ is called **optimal** if $ v_s(a) \\leq v_\\sigma(a) $ for all\n",
    "asset levels $ a $ and all alternative policies $ s \\in \\Sigma $.\n",
    "\n",
    "The function $ v^* $ defined by $ v^*(a) := \\sup_{\\sigma \\in \\Sigma} v_\\sigma(a) $\n",
    "is called the **value function**.\n",
    "\n",
    "Using this definition, we can alternatively say that a policy $ \\sigma $ is optimal\n",
    "if and only if $ v_\\sigma = v^* $.\n",
    "\n",
    "We know that we can find an optimal policy using dynamic programming and, in\n",
    "particular, the endogenous grid method (EGM).\n",
    "\n",
    "Now let’s look at another method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fbd2da",
   "metadata": {},
   "source": [
    "### The policy gradient approach\n",
    "\n",
    "The policy gradient approach starts by fixing an initial distribution $ F $ and\n",
    "trying to solve\n",
    "\n",
    "$$\n",
    "\\max_{\\sigma \\in \\Sigma} \\int v_\\sigma(a) F(d a)\n",
    "$$\n",
    "\n",
    "Working with this alternative objective transforms a dynamic programming problem\n",
    "into a regular optimization with a real-valued objective (the last display).\n",
    "\n",
    "Here we’ll focus on the case where $ F $ concentrates on a single point $ a_0 $, so\n",
    "the objective becomes\n",
    "\n",
    "$$\n",
    "\\max_{\\sigma \\in \\Sigma} M(\\sigma)\n",
    "    \\quad \\text{where} \\quad\n",
    "    M(\\sigma) := v_\\sigma(a_0)\n",
    "$$\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Does our choice of the initial condition $ a_0 $ matter in terms of delivering an\n",
    "optimal policy?\n",
    "\n",
    "The answer is, in general, [yes](https://arxiv.org/html/2411.11062v2).\n",
    "\n",
    "Essentially, we want the state to explore as much of the state space as\n",
    "possible.\n",
    "\n",
    "We’ll try to engineer this outcome in our choice of $ a_0 $.\n",
    "\n",
    "From here the approach is\n",
    "\n",
    "1. Replace $ \\Sigma $ with $ \\{\\sigma(\\cdot, \\theta) \\,:\\, \\theta \\in \\Theta\\} $\n",
    "  where $ \\sigma(\\cdot, \\theta) $ is an ANN with parameter vector $ \\theta $  \n",
    "1. Replace the objective function with $ M(\\theta) := v_{\\sigma(\\cdot, \\theta)} (a_0) $  \n",
    "1. Replace $ M $ with a Monte Carlo approximation $ \\hat M $  \n",
    "1. Use gradient ascent to maximize $ \\hat M(\\theta) $ over $ \\theta $.  \n",
    "\n",
    "\n",
    "In the last step we do\n",
    "\n",
    "$$\n",
    "\\theta_{n+1} = \\theta_n + \\lambda_n \\nabla \\hat M(\\theta_n)\n",
    "$$\n",
    "\n",
    "We compute $ \\hat M $ via\n",
    "\n",
    "$$\n",
    "\\hat M(\\theta)\n",
    "    = \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T-1} \\beta^t u(\\sigma(a^i_t, \\theta))\n",
    "$$\n",
    "\n",
    "Here $ a^i_0 $ is fixed at the given value $ a_0 $ for all $ i $ and\n",
    "\n",
    "$$\n",
    "a^i_{t+1} = R (a^i_t - \\sigma(a^i_t, \\theta)) + Y^i_{t+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f362035",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Before we get to policy gradient ascent, let’s set up a generic deep learning\n",
    "environment.\n",
    "\n",
    "The environment will work with an arbitrary loss function.\n",
    "\n",
    "Below, in each optimal savings application, we will specify the loss function as\n",
    "$ -\\hat M $, where $ \\hat M $ is the approximation to lifetime value defined above.\n",
    "\n",
    "Thus, minimizing loss in policy space means maximizing lifetime value (given fixed $ a_0 $).\n",
    "\n",
    "We store some fixed values that form part of the network training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90483357",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class Config(NamedTuple):\n",
    "    \"\"\"\n",
    "    Configuration and parameters for training the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    seed: int = 1234                         # Seed for network initialization\n",
    "    epochs: int = 400                        # No of training epochs\n",
    "    path_length: int = 200                   # Length of each consumption path\n",
    "    layer_sizes: tuple = (1, 6, 6, 6, 1)     # Network layer sizes\n",
    "    learning_rate: float = 0.001             # Constant learning rate\n",
    "    num_paths: int = 100                     # Number of paths to average over "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80eb84",
   "metadata": {},
   "source": [
    "We use a class called `LayerParams` to store parameters representing a single\n",
    "layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58ed87",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class LayerParams(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for one layer of the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    W: jnp.ndarray     # weights\n",
    "    b: jnp.ndarray     # biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0f237",
   "metadata": {},
   "source": [
    "The following function initializes a single layer of the network using Le Cun\n",
    "initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae6004",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def initialize_layer(\n",
    "        in_dim: int,                     # Input dimension for the layer\n",
    "        out_dim: int,                    # Output dimension for the layer\n",
    "        key: jax.Array                   # Random key for initialization\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a single layer of the network.\n",
    "    Use LeCun initialization.\n",
    "\n",
    "    \"\"\"\n",
    "    s = jnp.sqrt(1.0 / in_dim)\n",
    "    W = jax.random.normal(key, (in_dim, out_dim)) * s\n",
    "    b = jnp.zeros((out_dim,))\n",
    "    return LayerParams(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a40128",
   "metadata": {},
   "source": [
    "The next function builds an entire network, as represented by its parameters, by\n",
    "initializing layers and stacking them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525c79d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def initialize_network(\n",
    "        key: jax.Array,                  # Random key for initialization\n",
    "        layer_sizes: tuple               # Layer sizes (input, hidden..., output)\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Build a network by initializing all of the parameters.\n",
    "    A network is a list of LayerParams instances, each\n",
    "    containing a weight-bias pair (W, b).\n",
    "\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    # For all layers but the output layer\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        # Build the layer \n",
    "        key, subkey = jax.random.split(key)\n",
    "        layer = initialize_layer(\n",
    "            layer_sizes[i],      # in dimension for layer\n",
    "            layer_sizes[i + 1],  # out dimension for layer\n",
    "            subkey \n",
    "        )\n",
    "        # And add it to the parameter list\n",
    "        params.append(layer)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4bd65",
   "metadata": {},
   "source": [
    "Next we write a function to train the network by gradient *descent*, given a generic loss\n",
    "function.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">We use gradient descent rather than ascent because we’ll employ optax, which\n",
    "expects to be minimizing a loss function.\n",
    "\n",
    "To make this work, we’ll set the loss to $ - \\hat M(\\theta) $.\n",
    "\n",
    "Here’s the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27073e4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=('config', 'loss_fn'))\n",
    "def train_network(\n",
    "        config: Config,            # Configuration with training parameters\n",
    "        loss_fn: callable          # Loss function taking params, returning loss\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train a neural network using policy gradient ascent.\n",
    "\n",
    "    This is a generic training function that can be applied to different\n",
    "    models by providing an appropriate loss function.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize network parameters\n",
    "    key = random.PRNGKey(config.seed)\n",
    "    params = initialize_network(key, config.layer_sizes)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),  # Gradient clipping for stability\n",
    "        optax.adam(learning_rate=config.learning_rate)\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    # Training loop state\n",
    "    def step(i, state):\n",
    "        params, opt_state, best_value, best_params, value_history = state\n",
    "\n",
    "        # Compute value and gradients at existing parameterization\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        lifetime_value = -loss\n",
    "\n",
    "        # Update value history\n",
    "        value_history = value_history.at[i].set(lifetime_value)\n",
    "\n",
    "        # Track best parameters\n",
    "        is_best = lifetime_value > best_value\n",
    "        best_value = jnp.where(is_best, lifetime_value, best_value)\n",
    "        best_params = jax.tree.map(\n",
    "            lambda new, old: jnp.where(is_best, new, old),\n",
    "            params, best_params\n",
    "        )\n",
    "\n",
    "        # Update parameters using optimizer\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        return params, opt_state, best_value, best_params, value_history\n",
    "\n",
    "    # Run training loop\n",
    "    value_history = jnp.zeros(config.epochs)\n",
    "    initial_state = (params, opt_state, -jnp.inf, params, value_history)\n",
    "    final_state = jax.lax.fori_loop(\n",
    "        0, config.epochs, step, initial_state\n",
    "    )\n",
    "\n",
    "    # Extract results\n",
    "    _, _, best_value, best_params, value_history = final_state\n",
    "    return best_params, value_history, best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d238690",
   "metadata": {},
   "source": [
    "## Cake Eating Case\n",
    "\n",
    "We will start by tackling a very simple case, without any labor income, so that $ Y_t $ is\n",
    "always zero and\n",
    "\n",
    "$$\n",
    "a_{t+1} = R(a_t - c_t) \\qquad \\text{for all } t\n",
    "$$\n",
    "\n",
    "For this “cake-eating” model, the optimal policy is known to be\n",
    "\n",
    "$$\n",
    "c = \\kappa a, \n",
    "    \\quad \\text{where} \\quad\n",
    "    \\kappa := 1 - [\\beta R^{1-\\gamma}]^{1/\\gamma}\n",
    "$$\n",
    "\n",
    "We use this known exact solution to check our numerical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdea103",
   "metadata": {},
   "source": [
    "### Cake eating loss function\n",
    "\n",
    "We use a class called `CakeEatingModel` to store model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ac579",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class CakeEatingModel(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for the model.\n",
    "\n",
    "    \"\"\"\n",
    "    γ: float = 1.5\n",
    "    β: float = 0.96\n",
    "    R: float = 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fda97d",
   "metadata": {},
   "source": [
    "We use CRRA utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e06d66",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def u(c, γ):\n",
    "    \"\"\" Utility function. \"\"\"\n",
    "    c = jnp.maximum(c, 1e-10)\n",
    "    return c**(1 - γ) / (1 - γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbecd6",
   "metadata": {},
   "source": [
    "Now we provide a function that implements a consumption policy as a neural network, given the\n",
    "parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fa96f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def forward(\n",
    "        params: list,         # Network parameters (LayerParams list)\n",
    "        a: float              # Current asset level\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Evaluate neural network policy: maps a given asset level a to\n",
    "    consumption rate c/a by running a forward pass through the network.\n",
    "\n",
    "    \"\"\"\n",
    "    σ = jax.nn.selu          # Activation function\n",
    "    x = jnp.array((a,))      # Make state a 1D array\n",
    "    # Forward pass through network, without the last step\n",
    "    for W, b in params[:-1]:\n",
    "        x = σ(x @ W + b)\n",
    "    # Complete with sigmoid activation for consumption rate\n",
    "    W, b = params[-1]\n",
    "    # Direct output in [0, 0.99] range for stability\n",
    "    x = jax.nn.sigmoid(x @ W + b) * 0.99 \n",
    "    # Extract and return consumption rate\n",
    "    consumption_rate = x[0]\n",
    "    return consumption_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85809f7",
   "metadata": {},
   "source": [
    "The next function approximates lifetime value for the cake eating agent\n",
    "associated with a given policy, as represented by the parameters of a neural\n",
    "network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdb442",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=('path_length'))\n",
    "def compute_lifetime_value(\n",
    "        params: list,                # Network parameters\n",
    "        cake_eating_model: tuple,    # Model parameters (γ, β, R)\n",
    "        path_length: int             # Length of simulation path\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compute the lifetime value of a path generated from\n",
    "    the policy embedded in params and the initial condition a_0 = 1.\n",
    "\n",
    "    \"\"\"\n",
    "    γ, β, R = cake_eating_model\n",
    "    initial_a = 1.0\n",
    "\n",
    "    def update(t, state):\n",
    "        # Unpack and compute consumption given current assets\n",
    "        a, value, discount = state\n",
    "        consumption_rate = forward(params, a)\n",
    "        c = consumption_rate * a\n",
    "        # Update loop state and return it\n",
    "        a = R * (a - c)\n",
    "        value = value + discount * u(c, γ)\n",
    "        discount = discount * β\n",
    "        new_state = a, value, discount\n",
    "        return new_state\n",
    "\n",
    "    initial_value, initial_discount = 0.0, 1.0\n",
    "    initial_state = initial_a, initial_value, initial_discount\n",
    "    final_a, final_value, discount = jax.lax.fori_loop(\n",
    "        0, path_length, update, initial_state\n",
    "    )\n",
    "    return final_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cae6d",
   "metadata": {},
   "source": [
    "Here’s the loss function we will minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362bf5f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def loss_function(params, cake_eating_model, path_length):\n",
    "    \"\"\"\n",
    "    Loss is the negation of the lifetime value of the policy \n",
    "    identified by `params`.\n",
    "\n",
    "    \"\"\"\n",
    "    return -compute_lifetime_value(params, cake_eating_model, path_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84345cd",
   "metadata": {},
   "source": [
    "### Train and solve\n",
    "\n",
    "First we create an instance of the model and unpack names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c0b3e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = CakeEatingModel()\n",
    "γ, β, R = model.γ, model.β, model.R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a3c68",
   "metadata": {},
   "source": [
    "We test stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c6780",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "assert β * R**(1 - γ) < 1, \"Parameters fail stability test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9ab24",
   "metadata": {},
   "source": [
    "We compute the optimal consumption rate and lifetime value from the analytical\n",
    "expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf99708",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "κ = 1 - (β * R**(1 - γ))**(1/γ)\n",
    "print(f\"Optimal consumption rate = {κ:.4f}.\\n\")\n",
    "v_max = κ**(-γ) * u(1.0, γ)\n",
    "print(f\"Theoretical maximum lifetime value = {v_max:.4f}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffffc1",
   "metadata": {},
   "source": [
    "Now let’s train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef46fa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "config = Config(num_paths=1)\n",
    "\n",
    "# Create a loss function that has params as the only argument\n",
    "loss_fn = lambda params: loss_function(params, model, config.path_length)\n",
    "\n",
    "# Warmup to trigger JIT compilation\n",
    "print(\"Warming up JIT compilation...\")\n",
    "_ = train_network(config, loss_fn)\n",
    "\n",
    "start_time = time.time()\n",
    "params, value_history, best_value = train_network(config, loss_fn)\n",
    "best_value.block_until_ready()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBest value: {best_value:.4f}\")\n",
    "print(f\"Final value: {value_history[-1]:.4f}\")\n",
    "print(f\"Training time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cef37d",
   "metadata": {},
   "source": [
    "First we plot the evolution of lifetime value over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c175d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Plot learning progress\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(value_history, 'b-', linewidth=2)\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('policy value')\n",
    "ax.set_title('learning progress')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7952c0e",
   "metadata": {},
   "source": [
    "Next we compare the learned and optimal policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106c94a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "a_grid = jnp.linspace(0.01, 1.0, 1000)\n",
    "policy_vmap = jax.vmap(lambda a: forward(params, a))\n",
    "consumption_rate = policy_vmap(a_grid)\n",
    "# Compute actual consumption: c = (c/a) * a\n",
    "c_learned = consumption_rate * a_grid\n",
    "c_optimal = κ * a_grid\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(a_grid, c_optimal, lw=2, label='optimal')\n",
    "ax.plot(a_grid, c_learned, linestyle='--', lw=4, alpha=0.6, label='DL policy')\n",
    "ax.set_xlabel('assets')\n",
    "ax.set_ylabel('consumption')\n",
    "ax.set_title('Consumption policy')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6ae1e",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Let’s have a look at paths for consumption and assets under the learned and\n",
    "optimal policies.\n",
    "\n",
    "The figures below show that the learned policies are close to optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6609e33",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def simulate_consumption_path(\n",
    "        params,   # ANN-based policy identified by params\n",
    "        a_0,      # Initial condition\n",
    "        T=120     # Simulation length\n",
    "    ):\n",
    "    # Simulate consumption and asset paths using ANN\n",
    "    a = a_0\n",
    "    a_sim, c_sim = [a], [] \n",
    "    for t in range(T):\n",
    "        # Update policy path \n",
    "        c = forward(params, a) * a\n",
    "        c_sim.append(float(c))\n",
    "        a = R * (a - c)\n",
    "        a_sim.append(float(a))\n",
    "        if a <= 1e-10:\n",
    "            break\n",
    "\n",
    "    # Simulate consumption and asset paths using optimal policy\n",
    "    a = a_0\n",
    "    a_opt, c_opt = [a], [] \n",
    "    for t in range(T):\n",
    "        # Update optimal path\n",
    "        c = κ * a\n",
    "        c_opt.append(c)\n",
    "        a = R * (a - c)\n",
    "        a_opt.append(a)\n",
    "        if a <= 1e-10:\n",
    "            break\n",
    "\n",
    "    return a_sim, c_sim, a_opt, c_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07df2e",
   "metadata": {},
   "source": [
    "Let’s simulate and plot path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a01a77",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "a_sim, c_sim, a_opt, c_opt = simulate_consumption_path(params, a_0=1.0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(a_sim, lw=4, linestyle='--', label='learned policy')\n",
    "ax1.plot(a_opt, lw=2, label='optimal')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Assets')\n",
    "ax1.set_title('Assets over time')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(c_sim, lw=4, linestyle='--', label='learned policy')\n",
    "ax2.plot(c_opt, lw=2, label='optimal')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Consumption')\n",
    "ax2.set_title('Consumption over time')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f99c5b",
   "metadata": {},
   "source": [
    "## IFP Model\n",
    "\n",
    "Now let’s solve a model with IID stochastic labor income using deep learning.\n",
    "\n",
    "The set up was described at the start of this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca35ca",
   "metadata": {},
   "source": [
    "### JAX Implementation\n",
    "\n",
    "We start with a class called `IFP` that stores the model primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84048622",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class IFP(NamedTuple):\n",
    "    R: float                 # Gross interest rate R = 1 + r\n",
    "    β: float                 # Discount factor\n",
    "    γ: float                 # Preference parameter\n",
    "    z_mean: float            # Mean of log income shock\n",
    "    z_std: float             # Std dev of log income shock\n",
    "    z_samples: jnp.ndarray   # Std dev of log income shock\n",
    "\n",
    "\n",
    "def create_ifp(\n",
    "        r=0.01,\n",
    "        β=0.96,\n",
    "        γ=1.5,\n",
    "        z_mean=0.1,\n",
    "        z_std=0.1,\n",
    "        n_shocks=200,\n",
    "        seed=42\n",
    "    ):\n",
    "    R = 1 + r\n",
    "    assert R * β < 1, \"Stability condition violated.\"\n",
    "    key = random.PRNGKey(seed)\n",
    "    z_samples = z_mean + z_std * jax.random.normal(key, n_shocks)\n",
    "    return IFP(R, β, γ, z_mean, z_std, z_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634f443",
   "metadata": {},
   "source": [
    "### Solving the IID model using the EGM\n",
    "\n",
    "Since the shocks are IID, the optimal policy depends only on current assets $ a $.\n",
    "\n",
    "For the IID normal case, we need to compute the expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[u'(\\sigma(R s + Y))]\n",
    "$$\n",
    "\n",
    "where $ Z \\sim N(m, v) $ and $ Y = \\exp(Z) $.\n",
    "\n",
    "We approximate this expectation using Monte Carlo.\n",
    "\n",
    "Here is the EGM operator $ K $ for the IID case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005d95e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def K(\n",
    "        c_in: jnp.ndarray,       # Current consumption policy on endogenous grid\n",
    "        a_in: jnp.ndarray,       # Current endogenous asset grid\n",
    "        ifp: IFP,                # IFP model instance\n",
    "        s_grid: jnp.ndarray,     # Exogenous savings grid\n",
    "        n_shocks: int = 50       # Number of points for Monte Carlo integration\n",
    "    ):\n",
    "    \"\"\"\n",
    "    The Euler equation operator for the IFP model with IID shocks using EGM.\n",
    "\n",
    "    \"\"\"\n",
    "    R, β, γ, z_mean, z_std, z_samples = ifp\n",
    "    y_samples = jnp.exp(z_samples)\n",
    "    u_prime = lambda c: c**(-γ)\n",
    "    u_prime_inv = lambda c: c**(-1/γ)\n",
    "\n",
    "    def compute_c_i(s_i):\n",
    "        \"\"\"Compute consumption for savings level s_i.\"\"\"\n",
    "\n",
    "        # For each income realization, compute next period assets and consumption\n",
    "        def compute_mu_k(y_k):\n",
    "            next_a = R * s_i + y_k\n",
    "            # Interpolate to get consumption\n",
    "            next_c = jnp.interp(next_a, a_in, c_in)\n",
    "            return u_prime(next_c)\n",
    "\n",
    "        # Compute expectation over income shocks (Monte Carlo average)\n",
    "        mu_values = jax.vmap(compute_mu_k)(y_samples)\n",
    "        expectation = jnp.mean(mu_values)\n",
    "\n",
    "        # Invert to get consumption (handles s_i=0 case via smooth function)\n",
    "        c = u_prime_inv(β * R * expectation)\n",
    "\n",
    "        # For s_i = 0, consumption should be 0\n",
    "        return jnp.where(s_i == 0, 0.0, c)\n",
    "\n",
    "    # Compute consumption for all savings levels\n",
    "    c_out = jax.vmap(compute_c_i)(s_grid)\n",
    "    # Compute endogenous asset grid\n",
    "    a_out = c_out + s_grid\n",
    "\n",
    "    return c_out, a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67568db",
   "metadata": {},
   "source": [
    "Here’s the solver using time iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dff878",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def solve_model(\n",
    "        ifp: IFP,                # IFP model instance\n",
    "        s_grid: jnp.ndarray,     # Exogenous savings grid\n",
    "        n_shocks: int = 50,      # Number of income shock realizations\n",
    "        tol: float = 1e-5,       # Convergence tolerance\n",
    "        max_iter: int = 1000     # Maximum iterations\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Solve the IID model using time iteration with EGM.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize with consumption = assets (consume everything)\n",
    "    a_init = s_grid.copy()\n",
    "    c_init = s_grid.copy()\n",
    "    c_in, a_in = c_init, a_init\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        c_out, a_out = K(c_in, a_in, ifp, s_grid, n_shocks)\n",
    "\n",
    "        error = jnp.max(jnp.abs(c_out - c_in))\n",
    "\n",
    "        if error < tol:\n",
    "            print(f\"Converged in {i} iterations, error = {error:.2e}\")\n",
    "            break\n",
    "\n",
    "        c_in, a_in = c_out, a_out\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, error = {error:.2e}\")\n",
    "\n",
    "    return c_out, a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf934e1e",
   "metadata": {},
   "source": [
    "Let’s solve the model and plot the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309d445",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "ifp = create_ifp(z_mean=0.1, z_std=0.1)\n",
    "\n",
    "# Create savings grid\n",
    "s_grid = jnp.linspace(0, 10, 200)\n",
    "\n",
    "# Solve using EGM\n",
    "print(\"Solving IFP model using EGM...\\n\")\n",
    "c_egm, a_egm = solve_model(ifp, s_grid, n_shocks=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef533e2",
   "metadata": {},
   "source": [
    "Plot the optimal consumption policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c979804",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(a_egm, c_egm, lw=2, label='EGM solution')\n",
    "ax.set_xlabel('assets')\n",
    "ax.set_ylabel('consumption')\n",
    "ax.set_title('Optimal consumption policy (IFP model, EGM)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9a95a",
   "metadata": {},
   "source": [
    "### Solving the IID model with DL\n",
    "\n",
    "Since the shocks are IID, the policy depends only on current assets $ a $.\n",
    "\n",
    "We use the same network architecture as the deterministic case.\n",
    "\n",
    "The forward pass uses the `forward` function from the deterministic case.\n",
    "\n",
    "Here we implement lifetime value computation.\n",
    "\n",
    "The key is to simulate paths with IID normal income shocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb58df",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=('path_length', 'num_paths'))\n",
    "def compute_lifetime_value_ifp(\n",
    "        params: list,            # Neural network parameters\n",
    "        ifp: IFP,                # IFP model instance\n",
    "        path_length: int,        # Length of each simulated path\n",
    "        num_paths: int,          # Number of paths to simulate for averaging\n",
    "        key: jax.Array           # JAX random key for generating income shocks\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compute expected lifetime value by averaging over multiple\n",
    "    simulated paths.\n",
    "\n",
    "    \"\"\"\n",
    "    R, β, γ, z_mean, z_std, z_samples = ifp\n",
    "\n",
    "    def simulate_path(subkey):\n",
    "        \"\"\"Simulate a single path and return its lifetime value.\"\"\"\n",
    "        z_shocks = z_mean + z_std * jax.random.normal(subkey, path_length)\n",
    "        Y = jnp.exp(z_shocks)\n",
    "\n",
    "        def update(t, loop_state):\n",
    "            a, value, discount = loop_state\n",
    "            consumption_rate = forward(params, a)\n",
    "            c = consumption_rate * a\n",
    "            next_value = value + discount * u(c, γ)\n",
    "            next_a = R * (a - c) + Y[t]\n",
    "            next_discount = discount * β\n",
    "            return next_a, next_value, next_discount\n",
    "\n",
    "        initial_a = 10.0\n",
    "        initial_value = 0.0\n",
    "        initial_discount = 1.0\n",
    "        initial_state = (initial_a, initial_value, initial_discount)\n",
    "        final_a, final_value, final_discount = jax.lax.fori_loop(\n",
    "            0, path_length, update, initial_state\n",
    "        )\n",
    "\n",
    "        return final_value\n",
    "\n",
    "    # Generate keys for all paths\n",
    "    path_keys = jax.random.split(key, num_paths)\n",
    "\n",
    "    # Simulate all paths and average\n",
    "    values = jax.vmap(simulate_path)(path_keys)\n",
    "    return jnp.mean(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2230d",
   "metadata": {},
   "source": [
    "The loss function is the negation of the expected lifetime value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dca0a8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def loss_function_ifp(params, ifp, path_length, num_paths, key):\n",
    "    return -compute_lifetime_value_ifp(\n",
    "        params, ifp, path_length, num_paths, key\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9541b",
   "metadata": {},
   "source": [
    "Now let’s set up and train the network.\n",
    "\n",
    "We use the same `ifp` instance that was created for the EGM solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da263f8a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "key = random.PRNGKey(config.seed)\n",
    "\n",
    "print(\"Training IFP model with deep learning...\\n\")\n",
    "\n",
    "# Set up loss function to pass to train_network\n",
    "ifp_loss_fn = lambda params: loss_function_ifp(\n",
    "    params, ifp, config.path_length, config.num_paths, key\n",
    ")\n",
    "\n",
    "# Warmup to trigger JIT compilation\n",
    "print(\"Warming up JIT compilation...\")\n",
    "_ = train_network(config, ifp_loss_fn)\n",
    "\n",
    "start_time = time.time()\n",
    "ifp_params, ifp_value_history, best_ifp_value = train_network(\n",
    "    config, ifp_loss_fn\n",
    ")\n",
    "best_ifp_value.block_until_ready()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBest value: {best_ifp_value:.4f}\")\n",
    "print(f\"Final value: {ifp_value_history[-1]:.4f}\")\n",
    "print(f\"Training time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e210338",
   "metadata": {},
   "source": [
    "Plot the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44349cb0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ifp_value_history, linewidth=2)\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('policy value')\n",
    "ax.set_title('Learning progress')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e134c0",
   "metadata": {},
   "source": [
    "Compare EGM and DL solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b30a63",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Evaluate DL policy on asset grid\n",
    "a_grid_dl = jnp.linspace(0.01, 10.0, 200)\n",
    "policy_vmap = jax.vmap(lambda a: forward(ifp_params, a))\n",
    "consumption_rate_dl = policy_vmap(a_grid_dl)\n",
    "c_dl = consumption_rate_dl * a_grid_dl\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(a_egm, c_egm, lw=2, label='EGM solution')\n",
    "ax.plot(a_grid_dl, c_dl, linestyle='--', lw=4, alpha=0.6, label='DL solution')\n",
    "ax.set_xlabel('assets', fontsize=12)\n",
    "ax.set_ylabel('consumption', fontsize=12)\n",
    "ax.set_xlim(0, min(a_grid_dl[-1], a_egm[-1]))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218328c0",
   "metadata": {},
   "source": [
    "The fit is quite good."
   ]
  }
 ],
 "metadata": {
  "date": 1766815079.5418136,
  "filename": "ifp_dl.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Policy Gradient-Based Optimal Savings"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}