{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e08f7a2",
   "metadata": {},
   "source": [
    "# Adventures with Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f594ce6",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with JAX installed and access to a GPU.\n",
    "\n",
    "To run this lecture on [Google Colab](https://colab.research.google.com/), click on the “play” icon top right, select Colab, and set the runtime environment to include a GPU.\n",
    "\n",
    "To run this lecture on your own machine, you need to install [Google JAX](https://github.com/google/jax)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9e7d0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture gives a brief introduction to automatic differentiation using\n",
    "Google JAX.\n",
    "\n",
    "Automatic differentiation is one of the key elements of modern machine learning\n",
    "and artificial intelligence.\n",
    "\n",
    "As such it has attracted a great deal of investment and there are several\n",
    "powerful implementations available.\n",
    "\n",
    "One of the best of these is the automatic differentiation routines contained\n",
    "in JAX.\n",
    "\n",
    "While other software packages also offer this feature, the JAX version is\n",
    "particularly powerful because it integrates so well with other core\n",
    "components of JAX (e.g., JIT compilation and parallelization).\n",
    "\n",
    "As we will see in later lectures, automatic differentiation can be used not only\n",
    "for AI but also for many problems faced in mathematical modeling, such as\n",
    "multi-dimensional nonlinear optimization and root-finding problems.\n",
    "\n",
    "We need the following imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339a8e7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99e900",
   "metadata": {},
   "source": [
    "Checking for a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbac8c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a526b",
   "metadata": {},
   "source": [
    "## What is automatic differentiation?\n",
    "\n",
    "Autodiff is a technique for calculating derivatives on a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17fb2f6",
   "metadata": {},
   "source": [
    "### Autodiff is not finite differences\n",
    "\n",
    "The derivative of $ f(x) = \\exp(2x) $ is\n",
    "\n",
    "$$\n",
    "f'(x) = 2 \\exp(2x)\n",
    "$$\n",
    "\n",
    "A computer that doesn’t know how to take derivatives might approximate this with the finite difference ratio\n",
    "\n",
    "$$\n",
    "(Df)(x) := \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "where $ h $ is a small positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db0e13",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"Original function.\"\n",
    "    return np.exp(2 * x)\n",
    "\n",
    "def f_prime(x):\n",
    "    \"True derivative.\"\n",
    "    return 2 * np.exp(2 * x)\n",
    "\n",
    "def Df(x, h=0.1):\n",
    "    \"Approximate derivative (finite difference).\"\n",
    "    return (f(x + h) - f(x))/h\n",
    "\n",
    "x_grid = np.linspace(-2, 1, 200)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f_prime(x_grid), label=\"$f'$\")\n",
    "ax.plot(x_grid, Df(x_grid), label=\"$Df$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d472651",
   "metadata": {},
   "source": [
    "This kind of numerical derivative is often inaccurate and unstable.\n",
    "\n",
    "One reason is that\n",
    "\n",
    "$$\n",
    "\\frac{f(x+h) - f(x)}{h} \\approx \\frac{0}{0}\n",
    "$$\n",
    "\n",
    "Small numbers in the numerator and denominator causes rounding errors.\n",
    "\n",
    "The situation is exponentially worse in high dimensions / with higher order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158022a",
   "metadata": {},
   "source": [
    "### Autodiff is not symbolic calculus\n",
    "\n",
    "Symbolic calculus tries to use rules for differentiation to produce a single\n",
    "closed-form expression representing a derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea317c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "m, a, b, x = symbols('m a b x')\n",
    "f_x = (a*x + b)**m\n",
    "f_x.diff((x, 6))  # 6-th order derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a32a13",
   "metadata": {},
   "source": [
    "Symbolic calculus is not well suited to high performance\n",
    "computing.\n",
    "\n",
    "One disadvantage is that symbolic calculus cannot differentiate through control flow.\n",
    "\n",
    "Also, using symbolic calculus might involve redundant calculations.\n",
    "\n",
    "For example, consider\n",
    "\n",
    "$$\n",
    "(f g h)'\n",
    "    = (f' g + g' f) h + (f g) h'\n",
    "$$\n",
    "\n",
    "If we evaluate at $ x $, then we evalute $ f(x) $ and $ g(x) $ twice each.\n",
    "\n",
    "Also, computing $ f'(x) $ and $ f(x) $ might involve similar terms (e.g., $ (f(x) = \\exp(2x)' \\implies f'(x) = 2f(x) $) but this is not exploited in symbolic algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ccf49b",
   "metadata": {},
   "source": [
    "### Autodiff\n",
    "\n",
    "Autodiff produces functions that evaluates derivatives at numerical values\n",
    "passed in by the calling code, rather than producing a single symbolic\n",
    "expression representing the entire derivative.\n",
    "\n",
    "Derivatives are constructed by breaking calculations into component parts via the chain rule.\n",
    "\n",
    "The chain rule is applied until the point where the terms reduce to primitive functions that the program knows how to differentiate exactly (addition, subtraction, exponentiation, sine and cosine, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10007c21",
   "metadata": {},
   "source": [
    "## Some experiments\n",
    "\n",
    "Let’s start with some real-valued functions on $ \\mathbb R $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e565754",
   "metadata": {},
   "source": [
    "### A differentiable function\n",
    "\n",
    "Let’s test JAX’s auto diff with a relatively simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502e8f9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.sin(x) - 2 * jnp.cos(3 * x) * jnp.exp(- x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a38ac",
   "metadata": {},
   "source": [
    "We use `grad` to compute the gradient of a real-valued function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e352d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152bd8c",
   "metadata": {},
   "source": [
    "Let’s plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90579d3d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_grid = jnp.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6358b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, [f(x) for x in x_grid], label=\"$f$\")\n",
    "ax.plot(x_grid, [f_prime(x) for x in x_grid], label=\"$f'$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283e708",
   "metadata": {},
   "source": [
    "### Absolute value function\n",
    "\n",
    "What happens if the function is not differentiable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83409e00",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90036319",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b51612",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, [f(x) for x in x_grid], label=\"$f$\")\n",
    "ax.plot(x_grid, [f_prime(x) for x in x_grid], label=\"$f'$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb76d1",
   "metadata": {},
   "source": [
    "At the nondifferentiable point $ 0 $, `jax.grad` returns the right derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97561f64",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f_prime(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1eebd",
   "metadata": {},
   "source": [
    "### Differentiating through control flow\n",
    "\n",
    "Let’s try differentiating through some loops and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3315c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    def f1(x):\n",
    "        for i in range(2):\n",
    "            x *= 0.2 * x\n",
    "        return x\n",
    "    def f2(x):\n",
    "        x = sum((x**i + i) for i in range(3))\n",
    "        return x\n",
    "    y = f1(x) if x < 0 else f2(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82488c48",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f23782",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_grid = jnp.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b5884",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, [f(x) for x in x_grid], label=\"$f$\")\n",
    "ax.plot(x_grid, [f_prime(x) for x in x_grid], label=\"$f'$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5e40b",
   "metadata": {},
   "source": [
    "### Differentiating through a linear interpolation\n",
    "\n",
    "We can differentiate through linear interpolation, even though the function is not smooth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa4ad1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "xp = jnp.linspace(-5, 5, n)\n",
    "yp = jnp.cos(2 * xp)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, jnp.interp(x_grid, xp, yp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565e71a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f_prime = jax.grad(jnp.interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039dc19",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f_prime_vec = jax.vmap(f_prime, in_axes=(0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8502b2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f_prime_vec(x_grid, xp, yp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731fdc6",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let’s try implementing gradient descent.\n",
    "\n",
    "As a simple application, we’ll use gradient descent to solve for the OLS parameter estimates in simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a16986",
   "metadata": {},
   "source": [
    "### A function for gradient descent\n",
    "\n",
    "Here’s an implementation of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ee262",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def grad_descent(f,       # Function to be minimized\n",
    "                 args,    # Extra arguments to the function\n",
    "                 x0,      # Initial condition\n",
    "                 λ=0.1,   # Initial learning rate\n",
    "                 tol=1e-5, \n",
    "                 max_iter=1_000):\n",
    "    \"\"\"\n",
    "    Minimize the function f via gradient descent, starting from guess x0.\n",
    "\n",
    "    The learning rate is computed according to the Barzilai-Borwein method.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f_grad = jax.grad(f)\n",
    "    x = jnp.array(x0)\n",
    "    df = f_grad(x, args)\n",
    "    ϵ = tol + 1\n",
    "    i = 0\n",
    "    while ϵ > tol and i < max_iter:\n",
    "        new_x = x - λ * df\n",
    "        new_df = f_grad(new_x, args)\n",
    "        Δx = new_x - x\n",
    "        Δdf = new_df - df\n",
    "        λ = jnp.abs(Δx @ Δdf) / (Δdf @ Δdf)\n",
    "        ϵ = jnp.max(jnp.abs(Δx))\n",
    "        x, df = new_x, new_df\n",
    "        i += 1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73bb13",
   "metadata": {},
   "source": [
    "### Simulated data\n",
    "\n",
    "We’re going to test our gradient descent function my minimizing a sum of least squares in a regression problem.\n",
    "\n",
    "Let’s generate some simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0909f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "key = jax.random.PRNGKey(1234)\n",
    "x = jax.random.uniform(key, (n,))\n",
    "\n",
    "α, β, σ = 0.5, 1.0, 0.1  # Set the true intercept and slope.\n",
    "key, subkey = jax.random.split(key)\n",
    "ϵ = jax.random.normal(subkey, (n,))\n",
    "\n",
    "y = α * x + β + σ * ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974e8e5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53014c80",
   "metadata": {},
   "source": [
    "Let’s start by calculating the estimated slope and intercept using closed form solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc836d9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "mx = x.mean()\n",
    "my = y.mean()\n",
    "α_hat = jnp.sum((x - mx) * (y - my)) / jnp.sum((x - mx)**2)\n",
    "β_hat = my - α_hat * mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f9b16",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "α_hat, β_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f503d9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, α_hat * x + β_hat, 'k-')\n",
    "ax.text(0.1, 1.55, rf'$\\hat \\alpha = {α_hat:.3}$')\n",
    "ax.text(0.1, 1.50, rf'$\\hat \\beta = {β_hat:.3}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364759fc",
   "metadata": {},
   "source": [
    "### Minimizing squared loss by gradient descent\n",
    "\n",
    "Let’s see if we can get the same values with our gradient descent function.\n",
    "\n",
    "First we set up the least squares loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29010a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(params, data):\n",
    "    a, b = params\n",
    "    x, y = data\n",
    "    return jnp.sum((y - a * x - b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de764a1d",
   "metadata": {},
   "source": [
    "Now we minimize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff97b19",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p0 = jnp.zeros(2)  # Initial guess for α, β\n",
    "data = x, y\n",
    "α_hat, β_hat = grad_descent(loss, data, p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ec62e",
   "metadata": {},
   "source": [
    "Let’s plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af2873",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_grid = jnp.linspace(0, 1, 100)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_grid, α_hat * x_grid + β_hat, 'k-', alpha=0.6)\n",
    "ax.text(0.1, 1.55, rf'$\\hat \\alpha = {α_hat:.3}$')\n",
    "ax.text(0.1, 1.50, rf'$\\hat \\beta = {β_hat:.3}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30c554",
   "metadata": {},
   "source": [
    "Notice that we get the same estimates as we did from the closed form solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7617f",
   "metadata": {},
   "source": [
    "### Adding a squared term\n",
    "\n",
    "Now let’s try fitting a second order polynomial.\n",
    "\n",
    "Here’s our new loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0480a0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(params, data):\n",
    "    a, b, c = params\n",
    "    x, y = data\n",
    "    return jnp.sum((y - a * x**2 - b * x - c)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12f2c9",
   "metadata": {},
   "source": [
    "Now we’re minimizing in three dimensions.\n",
    "\n",
    "Let’s try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779f89f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p0 = jnp.zeros(3)\n",
    "α_hat, β_hat, γ_hat = grad_descent(loss, data, p0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_grid, α_hat * x_grid**2 + β_hat * x_grid + γ_hat, 'k-', alpha=0.6)\n",
    "ax.text(0.1, 1.55, rf'$\\hat \\alpha = {α_hat:.3}$')\n",
    "ax.text(0.1, 1.50, rf'$\\hat \\beta = {β_hat:.3}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a944",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd34fae",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "The function `jnp.polyval` evaluates polynomials.\n",
    "\n",
    "For example, if `len(p)` is 3, then `jnp.polyval(p, x)`  returns\n",
    "\n",
    "$$\n",
    "f(p, x) := p_0 x^2 + p_1 x + p_2\n",
    "$$\n",
    "\n",
    "Use this function for polynomial regression.\n",
    "\n",
    "The (empirical) loss becomes\n",
    "\n",
    "$$\n",
    "\\ell(p, x, y) \n",
    "    = \\sum_{i=1}^n (y_i - f(p, x_i))^2\n",
    "$$\n",
    "\n",
    "Set $ k=4 $ and set the initial guess of `params` to `jnp.zeros(k)`.\n",
    "\n",
    "Use gradient descent to find the array `params` that minimizes the loss\n",
    "function and plot the result (following the examples above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c4c57",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 3.1](https://jax.quantecon.org/#auto_ex1)\n",
    "\n",
    "Here’s one solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccaa0e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def loss(params, data):\n",
    "    x, y = data\n",
    "    return jnp.sum((y - jnp.polyval(params, x))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf423a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "p0 = jnp.zeros(k)\n",
    "p_hat = grad_descent(loss, data, p0)\n",
    "print('Estimated parameter vector:')\n",
    "print(p_hat)\n",
    "print('\\n\\n')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_grid, jnp.polyval(p_hat, x_grid), 'k-', alpha=0.6)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "date": 1729509368.7237475,
  "filename": "autodiff.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Adventures with Autodiff"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}