{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db56db53",
   "metadata": {},
   "source": [
    "# Neural Network Regression with JAX and Optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb0d9b",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with JAX installed and access to a GPU.\n",
    "\n",
    "To run this lecture on [Google Colab](https://colab.research.google.com/), click on the “play” icon top right, select Colab, and set the runtime environment to include a GPU.\n",
    "\n",
    "To run this lecture on your own machine, you need to install [Google JAX](https://github.com/google/jax).\n",
    "\n",
    "In a [previous lecture](https://jax.quantecon.org/keras.html), we showed how to implement regression using a neural network via the popular deep learning library [Keras](https://keras.io/).\n",
    "\n",
    "In this lecture, we solve the same problem directly, using JAX operations rather than relying on the Keras frontend.\n",
    "\n",
    "The objective is to understand the nuts and bolts of the exercise better, as\n",
    "well as to explore more features of JAX.\n",
    "\n",
    "The lecture proceeds in three stages:\n",
    "\n",
    "1. We repeat the Keras exercise, to give ourselves a benchmark.  \n",
    "1. We solve the same problem in pure JAX, using pytree operations and gradient descent.  \n",
    "1. We solve the same problem using a combination of JAX and [Optax](https://optax.readthedocs.io/en/latest/index.html), an optimization library build for JAX.  \n",
    "\n",
    "\n",
    "We begin with imports and installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088f126",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4ec64",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf0efe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773bfb8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'jax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5a33d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc043384",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Let’s hardcode some of the learning-related constants we’ll use across all\n",
    "implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef34ad5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4000           # Number of passes through the data set\n",
    "DATA_SIZE = 400         # Sample size\n",
    "NUM_LAYERS = 4          # Depth of the network\n",
    "OUTPUT_DIM = 10         # Output dimension of input and hidden layers\n",
    "LEARNING_RATE = 0.001   # Learning rate for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408308b4",
   "metadata": {},
   "source": [
    "The next piece of code is repeated from [our Keras lecture](https://jax.quantecon.org/keras.html) and generates\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bee38e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def generate_data(x_min=0,           \n",
    "                  x_max=5,          \n",
    "                  data_size=DATA_SIZE,\n",
    "                  seed=1234): # Default size for dataset\n",
    "    np.random.seed(seed)\n",
    "    x = np.linspace(x_min, x_max, num=data_size)\n",
    "    ϵ = 0.2 * np.random.randn(data_size)\n",
    "    y = x**0.5 + np.sin(x) + ϵ\n",
    "    # Return observations as column vectors \n",
    "    x, y = [np.reshape(z, (data_size, 1)) for z in (x, y)]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a4cb3",
   "metadata": {},
   "source": [
    "## Training with Keras\n",
    "\n",
    "We repeat the Keras training exercise from [our Keras lecture](https://jax.quantecon.org/keras.html) as a\n",
    "benchmark.\n",
    "\n",
    "The code is essentially the same, although written slightly more succinctly.\n",
    "\n",
    "Here is a function to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0278f1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_keras_model(num_layers=NUM_LAYERS, \n",
    "                      activation_function='tanh'):\n",
    "    model = Sequential()\n",
    "    # Add layers to the network sequentially, from inputs towards outputs\n",
    "    for i in range(NUM_LAYERS-1):\n",
    "        model.add(\n",
    "           Dense(units=OUTPUT_DIM, \n",
    "                 activation=activation_function)\n",
    "           )\n",
    "    # Add a final layer that maps to a scalar value, for regression.\n",
    "    model.add(Dense(units=1))\n",
    "    # Embed training configurations\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(), \n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec0b03",
   "metadata": {},
   "source": [
    "Here is a function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9ff67",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def train_keras_model(model, x, y, x_validate, y_validate):\n",
    "    print(f\"Training NN using Keras.\")\n",
    "    start_time = time()\n",
    "    training_history = model.fit(\n",
    "        x, y, \n",
    "        batch_size=max(x.shape), \n",
    "        verbose=0,\n",
    "        epochs=EPOCHS, \n",
    "        validation_data=(x_validate, y_validate)\n",
    "    )\n",
    "    elapsed = time() - start_time\n",
    "    mse = model.evaluate(x_validate, y_validate, verbose=2)\n",
    "    print(f\"Trained Keras model in {elapsed:.2f} seconds with final MSE on validation data = {mse}\")\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759da06",
   "metadata": {},
   "source": [
    "The next function visualizes the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e5762",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_keras_output(model, x, y, x_validate, y_validate):\n",
    "    y_predict = model.predict(x_validate, verbose=2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    ax.plot(x, y_predict, label=\"fitted model\", color='black')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852c1b5",
   "metadata": {},
   "source": [
    "Here’s a function to run all the routines above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adad11",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def keras_run_all():\n",
    "    model = build_keras_model()\n",
    "    x, y = generate_data()\n",
    "    x_validate, y_validate = generate_data()\n",
    "    model, training_history = train_keras_model(\n",
    "            model, x, y, x_validate, y_validate\n",
    "    )\n",
    "    plot_keras_output(model, x, y, x_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01524133",
   "metadata": {},
   "source": [
    "Let’s put it to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7434627",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "keras_run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14f22d",
   "metadata": {},
   "source": [
    "We’ve seen this figure before and we note the relatively low final MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63eddc9",
   "metadata": {},
   "source": [
    "## Training with JAX\n",
    "\n",
    "For the JAX implementation, we need to construct the network ourselves, as a map\n",
    "from inputs to outputs.\n",
    "\n",
    "We’ll use the same network structure we used for the Keras implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425293c3",
   "metadata": {},
   "source": [
    "### Background and set up\n",
    "\n",
    "The neural network as the form\n",
    "\n",
    "$$\n",
    "f(\\theta, x) \n",
    "    = (A_3 \\circ \\sigma \\circ A_2 \\circ \\sigma \\circ A_1 \\circ \\sigma \\circ A_0)(x)\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $ x $ is a scalar input – a point on the horizontal axis in the Keras estimation above,  \n",
    "- $ \\circ $ means composition of maps,  \n",
    "- $ \\sigma $ is the activation function – in our case, $ \\tanh $, and  \n",
    "- $ A_i $ represents the affine map $ A_i x = W_i x + b_i $.  \n",
    "\n",
    "\n",
    "Each matrix $ W_i $ is called a **weight matrix** and each vector $ b_i $ is called **bias** term.\n",
    "\n",
    "The symbol $ \\theta $ represents the entire collection of parameters:\n",
    "\n",
    "$$\n",
    "\\theta = (W_0, b_0, W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "$$\n",
    "\n",
    "In fact, when we implement the affine map $ A_i x = W_i x + b_i $, we will work\n",
    "with row vectors rather than column vectors, so that\n",
    "\n",
    "- $ x $ and $ b_i $ are stored as row vectors, and  \n",
    "- the mapping is executed by JAX via the expression `x @ W + b`.  \n",
    "\n",
    "\n",
    "We work with row vectors because Python numerical operations are row-major rather than column-major, so that row-based operations tend to be more efficient.\n",
    "\n",
    "Here’s a function to initialize parameters.\n",
    "\n",
    "The parameter “vector” `θ`  will be stored as a list of dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e340ec7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def initialize_params(seed=1234):\n",
    "    \"\"\"\n",
    "    Generate an initial parameterization for a feed forward neural network with\n",
    "    number of layers = NUM_LAYERS.  Each of the hidden layers have OUTPUT_DIM\n",
    "    units.\n",
    "    \"\"\"\n",
    "    k = OUTPUT_DIM\n",
    "    shapes = (\n",
    "        (1, k),  # W_0.shape\n",
    "        (k, k),  # W_1.shape\n",
    "        (k, k),  # W_2.shape\n",
    "        (k, 1)   # W_3.shape\n",
    "    )   \n",
    "    np.random.seed(seed)\n",
    "    # A function to generate weight matrices\n",
    "    def w_init(m, n):\n",
    "        return np.random.normal(size=(m, n)) * np.sqrt(2 / m)\n",
    "    # Build list of dicts, each containing a (weight, bias) pair\n",
    "    θ = []\n",
    "    for w_shape in shapes:\n",
    "        m, n = w_shape\n",
    "        θ.append(dict(W=w_init(m, n), b=np.ones((1, n))) )\n",
    "    return θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abe5a4",
   "metadata": {},
   "source": [
    "Wait, you say!\n",
    "\n",
    "Shouldn’t we concatenate the elements of $ \\theta $ into some kind of big array, so that we can do autodiff with respect to this array?\n",
    "\n",
    "Actually we don’t need to, as will become clear below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9211d2",
   "metadata": {},
   "source": [
    "### Coding the network\n",
    "\n",
    "Here’s our implementation of $ f $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71451ea",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(θ, x, σ=jnp.tanh):\n",
    "    \"\"\"\n",
    "    Perform a forward pass over the network to evaluate f(θ, x).\n",
    "    The state x is stored and iterated on as a row vector.\n",
    "    \"\"\"\n",
    "    *hidden, last = θ\n",
    "    for layer in hidden:\n",
    "        W, b = layer['W'], layer['b']\n",
    "        x = σ(x @ W + b)\n",
    "    W, b = last['W'], last['b']\n",
    "    x = x @ W + b\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6dc5d",
   "metadata": {},
   "source": [
    "The function $ f $ is appropriately vectorized, so that we can pass in the entire\n",
    "set of input observations as `x` and return the predicted vector of outputs `y_hat = f(θ, x)`\n",
    "corresponding  to each data point.\n",
    "\n",
    "The loss function is mean squared error, the same as the Keras case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3acb10",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(θ, x, y):\n",
    "    \"Loss is mean squared error.\"\n",
    "    return jnp.mean((f(θ, x) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aab19d",
   "metadata": {},
   "source": [
    "We’ll use its gradient to do stochastic gradient descent.\n",
    "\n",
    "(Technically, we will be doing gradient descent, rather than stochastic\n",
    "gradient descent, since will not randomize over sample points when we\n",
    "evaluate the gradient.)\n",
    "\n",
    "The gradient below is with respect to the first argument `θ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ad2ab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "loss_gradient = jax.jit(jax.grad(loss_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0c826",
   "metadata": {},
   "source": [
    "The line above seems kind of magical, since we are differentiating with respect\n",
    "to a parameter “vector” stored as a list of dictionaries containing arrays.\n",
    "\n",
    "How can we differentiate with respect to such a complex object?\n",
    "\n",
    "The answer is that the list of dictionaries is treated internally as a\n",
    "[pytree](https://docs.jax.dev/en/latest/pytrees.html).\n",
    "\n",
    "The JAX function `grad` understands how to\n",
    "\n",
    "1. extract the individual arrays (the ``leaves’’ of the tree),  \n",
    "1. compute derivatives with respect to each one, and  \n",
    "1. pack the resulting derivatives into a pytree with the same structure as the parameter vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f272a50c",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Using the above code, we can now write our rule for updating the parameters via gradient descent, which is the\n",
    "algorithm we covered in our [lecture on autodiff](https://jax.quantecon.org/autodiff.html).\n",
    "\n",
    "In this case, however, to keep things as simple as possible, we’ll use a fixed learning rate for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3952b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_parameters(θ, x, y):\n",
    "    λ = LEARNING_RATE \n",
    "    gradient = loss_gradient(θ, x, y)\n",
    "    θ = jax.tree.map(lambda p, g: p - λ * g, θ, gradient)\n",
    "    return θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e17e45",
   "metadata": {},
   "source": [
    "We are implementing the gradient descent update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ec5a2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "    new_params = current_params - learning_rate * gradient_of_loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518913b",
   "metadata": {},
   "source": [
    "This is nontrivial for a complex structure such as a neural network, so how is\n",
    "it done?\n",
    "\n",
    "The key line in the function above is `Θ = jax.tree.map(lambda p, g: p - λ * g, θ, gradient)`.\n",
    "\n",
    "The `jax.tree.map` function understands `θ` and `gradient` as pytrees of the\n",
    "same structure and executes `p - λ * g` on the corresponding leaves of the pair\n",
    "of trees.\n",
    "\n",
    "This means that each weight matrix and bias vector is updated by gradient\n",
    "descent, exactly as required.\n",
    "\n",
    "Here’s code that puts this all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5fcb0a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def train_jax_model(θ, x, y, x_validate, y_validate):\n",
    "    \"\"\"\n",
    "    Train model using gradient descent via JAX autodiff.\n",
    "    \"\"\"\n",
    "    training_loss = np.empty(EPOCHS)\n",
    "    validation_loss = np.empty(EPOCHS)\n",
    "    for i in range(EPOCHS):\n",
    "        training_loss[i] = loss_fn(θ, x, y)\n",
    "        validation_loss[i] = loss_fn(θ, x_validate, y_validate)\n",
    "        θ = update_parameters(θ, x, y)\n",
    "    return θ, training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df878e2b",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Let’s run our code and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c368fa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "θ = initialize_params()\n",
    "x, y = generate_data()\n",
    "x_validate, y_validate = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add348b0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "θ, training_loss, validation_loss = train_jax_model(\n",
    "    θ, x, y, x_validate, y_validate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b01fa",
   "metadata": {},
   "source": [
    "This figure shows MSE across iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032f673",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(EPOCHS), validation_loss, label='validation loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f8993",
   "metadata": {},
   "source": [
    "Let’s check the final MSE on the validation data, at the estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23edda7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Final MSE on test data set = {loss_fn(θ, x_validate, y_validate)}.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0cdfc",
   "metadata": {},
   "source": [
    "This MSE is not as low as we got for Keras, but we did quite well given how\n",
    "simple our implementation is.\n",
    "\n",
    "Here’s a visualization of the quality of our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95678597",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.flatten(), f(θ, x).flatten(), \n",
    "        label=\"fitted model\", color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea487a4",
   "metadata": {},
   "source": [
    "## JAX plus Optax\n",
    "\n",
    "Our hand-coded optimization routine above was quite effective, but in practice\n",
    "we might wish to use an optimization library written for JAX.\n",
    "\n",
    "One such library is [Optax](https://optax.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2003cd1",
   "metadata": {},
   "source": [
    "### Optax with SGD\n",
    "\n",
    "Here’s a training routine using Optax’s stochastic gradient descent solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f42e3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def train_jax_optax(θ, x, y):\n",
    "    solver = optax.sgd(learning_rate=LEARNING_RATE)\n",
    "    opt_state = solver.init(θ)\n",
    "    for _ in range(EPOCHS):\n",
    "        grad = loss_gradient(θ, x, y)\n",
    "        updates, opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ = optax.apply_updates(θ, updates)\n",
    "    return θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb39cc",
   "metadata": {},
   "source": [
    "Let’s try running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931141b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Reset parameter vector\n",
    "θ = initialize_params()\n",
    "# Train network\n",
    "%time θ = train_jax_optax(θ, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc5470",
   "metadata": {},
   "source": [
    "The resulting MSE is the same as our hand-coded routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2cffb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Completed training JAX model using Optax with SGD.\n",
    "Final MSE on test data set = {loss_fn(θ, x_validate, y_validate)}.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76466d2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.flatten(), f(θ, x).flatten(), \n",
    "        label=\"fitted model\", color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fc5b7",
   "metadata": {},
   "source": [
    "### Optax with ADAM\n",
    "\n",
    "We can also consider using a slightly more sophisticated gradient-based method,\n",
    "such as [ADAM](https://arxiv.org/pdf/1412.6980).\n",
    "\n",
    "You will notice that the syntax for using this alternative optimizer is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631b11c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def train_jax_optax(θ, x, y):\n",
    "    solver = optax.adam(learning_rate=LEARNING_RATE)\n",
    "    opt_state = solver.init(θ)\n",
    "    for _ in range(EPOCHS):\n",
    "        grad = loss_gradient(θ, x, y)\n",
    "        updates, opt_state = solver.update(grad, opt_state, θ)\n",
    "        θ = optax.apply_updates(θ, updates)\n",
    "    return θ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239a4a1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Reset parameter vector\n",
    "θ = initialize_params()\n",
    "# Train network\n",
    "%time θ = train_jax_optax(θ, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700f976",
   "metadata": {},
   "source": [
    "Here’s the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cf86b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Completed training JAX model using Optax with ADAM.\n",
    "Final MSE on test data set = {loss_fn(θ, x_validate, y_validate)}.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2a1f5",
   "metadata": {},
   "source": [
    "Here’s a visualization of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94e790",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.flatten(), f(θ, x).flatten(), \n",
    "        label=\"fitted model\", color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "date": 1747720850.4411862,
  "filename": "jax_nn.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Neural Network Regression with JAX and Optax"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}