{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a745c261",
   "metadata": {},
   "source": [
    "# Job Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad119b",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with JAX installed and access to a GPU.\n",
    "\n",
    "To run this lecture on [Google Colab](https://colab.research.google.com/), click on the “play” icon top right, select Colab, and set the runtime environment to include a GPU.\n",
    "\n",
    "To run this lecture on your own machine, you need to install [Google JAX](https://github.com/google/jax).\n",
    "\n",
    "In this lecture we study a basic infinite-horizon job search with Markov wage\n",
    "draws\n",
    "\n",
    "The exercise at the end asks you to add recursive preferences and compare\n",
    "the result.\n",
    "\n",
    "In addition to what’s in Anaconda, this lecture will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a17665",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aaf7ba",
   "metadata": {},
   "source": [
    "We use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f4f14",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3c2c3",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We study an elementary model where\n",
    "\n",
    "- jobs are permanent  \n",
    "- unemployed workers receive current compensation $ c $  \n",
    "- the wage offer distribution $ \\{W_t\\} $ is Markovian  \n",
    "- the horizon is infinite  \n",
    "- an unemployment agent discounts the future via discount factor $ \\beta \\in (0,1) $  \n",
    "\n",
    "\n",
    "The wage process obeys\n",
    "\n",
    "$$\n",
    "W_{t+1} = \\rho W_t + \\nu Z_{t+1},\n",
    "    \\qquad \\{Z_t\\} \\text{ is IID and } N(0, 1)\n",
    "$$\n",
    "\n",
    "We discretize this using Tauchen’s method to produce a stochastic matrix $ P $\n",
    "\n",
    "Since jobs are permanent, the return to accepting wage offer $ w $ today is\n",
    "\n",
    "$$\n",
    "w + \\beta w + \\beta^2 w + \\cdots = \\frac{w}{1-\\beta}\n",
    "$$\n",
    "\n",
    "The Bellman equation is\n",
    "\n",
    "$$\n",
    "v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "We solve this model using value function iteration.\n",
    "\n",
    "Let’s set up a `namedtuple` to store information needed to solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b75f4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b75b0",
   "metadata": {},
   "source": [
    "The function below holds default values and populates the namedtuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90939acf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def create_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0        # unemployment compensation\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return Model(n, w_vals, P, β, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f34ff",
   "metadata": {},
   "source": [
    "Here’s the Bellman operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cc53f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β E v} with \n",
    "\n",
    "        e(w) = w / (1-β) and (Ev)(w) = E_w[ v(W')]\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    h = c + β * P @ v\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5197f9",
   "metadata": {},
   "source": [
    "The next function computes the optimal policy under the assumption that $ v $ is\n",
    "the value function.\n",
    "\n",
    "The policy takes the form\n",
    "\n",
    "$$\n",
    "\\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$\n",
    "\n",
    "Here $ \\mathbf 1 $ is an indicator function.\n",
    "\n",
    "The statement above means that the worker accepts ($ \\sigma(w) = 1 $) when the value of stopping\n",
    "is higher than the value of continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f89102",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_greedy(v, model):\n",
    "    \"\"\"Get a v-greedy policy.\"\"\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + β * P @ v\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843939d",
   "metadata": {},
   "source": [
    "Here’s a routine for value function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f8c83",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"\"\"Solve the infinite-horizon Markov job search model by VFI.\"\"\"\n",
    "\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy(v_star, model)\n",
    "    return v_star, σ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c01a8",
   "metadata": {},
   "source": [
    "### Computing the solution\n",
    "\n",
    "Let’s set up and solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e232d8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = create_js_model()\n",
    "n, w_vals, P, β, c = model\n",
    "\n",
    "%time v_star, σ_star = vfi(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d5a59",
   "metadata": {},
   "source": [
    "We run it again to eliminate compile time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079eb9fc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%time v_star, σ_star = vfi(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ff87a",
   "metadata": {},
   "source": [
    "We compute the reservation wage as the first $ w $ such that $ \\sigma(w)=1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae619900",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "res_wage = w_vals[jnp.searchsorted(σ_star, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901fd14",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star, alpha=0.8, label=\"value function\")\n",
    "ax.vlines((res_wage,), 150, 400, 'k', ls='--', label=\"reservation wage\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fc0be",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734d598",
   "metadata": {},
   "source": [
    "## Exercise 9.1\n",
    "\n",
    "In the setting above, the agent is risk-neutral vis-a-vis future utility risk.\n",
    "\n",
    "Now solve the same problem but this time assuming that the agent has risk-sensitive\n",
    "preferences, which are a type of nonlinear recursive preferences.\n",
    "\n",
    "The Bellman equation becomes\n",
    "\n",
    "$$\n",
    "v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, \n",
    "            c + \\frac{\\beta}{\\theta}\n",
    "            \\ln \\left[ \n",
    "                      \\sum_{w'} \\exp(\\theta v(w')) P(w, w')\n",
    "                \\right]\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "When $ \\theta < 0 $ the agent is risk sensitive.\n",
    "\n",
    "Solve the model when $ \\theta = -0.1 $ and compare your result to the risk neutral\n",
    "case.\n",
    "\n",
    "Try to interpret your result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd7248",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 9.1](https://jax.quantecon.org/#job_search_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179ebe9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "RiskModel = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c', 'θ'))\n",
    "\n",
    "def create_risk_sensitive_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "        θ=-0.1       # risk parameter\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return RiskModel(n, w_vals, P, β, c, θ)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def T_rs(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β R v} with \n",
    "\n",
    "        e(w) = w / (1-β) and\n",
    "\n",
    "        (Rv)(w) = (1/θ) ln{E_w[ exp(θ v(W'))]}\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_greedy_rs(v, model):\n",
    "    \" Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ\n",
    "\n",
    "\n",
    "\n",
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T_rs(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy_rs(v_star, model)\n",
    "    return v_star, σ_star\n",
    "\n",
    "\n",
    "\n",
    "model_rs = create_risk_sensitive_js_model()\n",
    "\n",
    "n, w_vals, P, β, c, θ = model_rs\n",
    "\n",
    "%time v_star_rs, σ_star_rs = vfi(model_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254809c",
   "metadata": {},
   "source": [
    "We run it again to eliminate the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6230a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%time v_star_rs, σ_star_rs = vfi(model_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e913b9f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "res_wage_rs = w_vals[jnp.searchsorted(σ_star_rs, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1419c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star,  alpha=0.8, label=\"RN $v$\")\n",
    "ax.plot(w_vals, v_star_rs, alpha=0.8, label=\"RS $v$\")\n",
    "ax.vlines((res_wage,), 150, 400,  ls='--', color='darkblue', alpha=0.5, label=r\"RV $\\bar w$\")\n",
    "ax.vlines((res_wage_rs,), 150, 400, ls='--', color='orange', alpha=0.5, label=r\"RS $\\bar w$\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfb600",
   "metadata": {},
   "source": [
    "The figure shows that the reservation wage under risk sensitive preferences (RS $ \\bar w $) shifts down.\n",
    "\n",
    "This makes sense – the agent does not like risk and hence is more inclined to\n",
    "accept the current offer, even when it’s lower."
   ]
  }
 ],
 "metadata": {
  "date": 1718254771.195203,
  "filename": "job_search.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Job Search"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}