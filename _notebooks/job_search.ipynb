{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e34dfb0",
   "metadata": {},
   "source": [
    "# Job Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eac2da",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with JAX installed and access to a GPU.\n",
    "\n",
    "To run this lecture on [Google Colab](https://colab.research.google.com/), click on the “play” icon top right, select Colab, and set the runtime environment to include a GPU.\n",
    "\n",
    "To run this lecture on your own machine, you need to install [Google JAX](https://github.com/google/jax).\n",
    "\n",
    "In this lecture we study a basic infinite-horizon job search problem with Markov wage\n",
    "draws\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For background on infinite horizon job search see, e.g., [DP1](https://dp.quantecon.org/).\n",
    "\n",
    "The exercise at the end asks you to add risk-sensitive preferences and see how\n",
    "the main results change.\n",
    "\n",
    "In addition to what’s in Anaconda, this lecture will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fcbae",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961870a",
   "metadata": {},
   "source": [
    "We use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298b307",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22efebd",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We study an elementary model where\n",
    "\n",
    "- jobs are permanent  \n",
    "- unemployed workers receive current compensation $ c $  \n",
    "- the horizon is infinite  \n",
    "- an unemployment agent discounts the future via discount factor $ \\beta \\in (0,1) $  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52146a57",
   "metadata": {},
   "source": [
    "### Set up\n",
    "\n",
    "At the start of each period, an unemployed worker receives wage offer $ W_t $.\n",
    "\n",
    "To build a wage offer process we consider the dynamics\n",
    "\n",
    "$$\n",
    "W_{t+1} = \\rho W_t + \\nu Z_{t+1}\n",
    "$$\n",
    "\n",
    "where $ (Z_t)_{t \\geq 0} $ is IID and standard normal.\n",
    "\n",
    "We then discretize this wage process using Tauchen’s method to produce a stochastic matrix $ P $.\n",
    "\n",
    "Successive wage offers are drawn from $ P $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b6d01",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "Since jobs are permanent, the return to accepting wage offer $ w $ today is\n",
    "\n",
    "$$\n",
    "w + \\beta w + \\beta^2 w + \n",
    "    \\cdots = \\frac{w}{1-\\beta}\n",
    "$$\n",
    "\n",
    "The Bellman equation is\n",
    "\n",
    "$$\n",
    "v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "We solve this model using value function iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab828a8",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Let’s set up a `namedtuple` to store information needed to solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f41a49",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6d540",
   "metadata": {},
   "source": [
    "The function below holds default values and populates the `namedtuple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c22c38",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def create_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), jnp.array(mc.P)\n",
    "    return Model(n, w_vals, P, β, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413fa00",
   "metadata": {},
   "source": [
    "Let’s test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fedac04",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = create_js_model(β=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6070e0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49393e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model.β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e86109",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model.w_vals.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0dfcd",
   "metadata": {},
   "source": [
    "Here’s the Bellman operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed351574",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β E v} with \n",
    "\n",
    "        e(w) = w / (1-β) and (Ev)(w) = E_w[ v(W')]\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    h = c + β * P @ v\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c12f7",
   "metadata": {},
   "source": [
    "The next function computes the optimal policy under the assumption that $ v $ is\n",
    "the value function.\n",
    "\n",
    "The policy takes the form\n",
    "\n",
    "$$\n",
    "\\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$\n",
    "\n",
    "Here $ \\mathbf 1 $ is an indicator function.\n",
    "\n",
    "- $ \\sigma(w) = 1 $ means stop  \n",
    "- $ \\sigma(w) = 0 $ means continue.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59915125",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_greedy(v, model):\n",
    "    \"Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + β * P @ v\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f4b5d",
   "metadata": {},
   "source": [
    "Here’s a routine for value function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271ad82",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy(v_star, model)\n",
    "    return v_star, σ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a39c0",
   "metadata": {},
   "source": [
    "## Computing the solution\n",
    "\n",
    "Let’s set up and solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c7c7c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = create_js_model()\n",
    "n, w_vals, P, β, c = model\n",
    "\n",
    "v_star, σ_star = vfi(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b944d0",
   "metadata": {},
   "source": [
    "Here’s the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68604de",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(σ_star)\n",
    "ax.set_xlabel(\"wage values\")\n",
    "ax.set_ylabel(\"optimal choice (stop=1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bd774",
   "metadata": {},
   "source": [
    "We compute the reservation wage as the first $ w $ such that $ \\sigma(w)=1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b9c60",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "stop_indices = jnp.where(σ_star == 1)\n",
    "stop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027af34",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "res_wage_index = min(stop_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b795c27",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "res_wage = w_vals[res_wage_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b62518",
   "metadata": {},
   "source": [
    "Here’s a joint plot of the value function and the reservation wage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5501b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star, alpha=0.8, label=\"value function\")\n",
    "ax.vlines((res_wage,), 150, 400, 'k', ls='--', label=\"reservation wage\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5373f1",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc0482",
   "metadata": {},
   "source": [
    "## Exercise 10.1\n",
    "\n",
    "In the setting above, the agent is risk-neutral vis-a-vis future utility risk.\n",
    "\n",
    "Now solve the same problem but this time assuming that the agent has risk-sensitive\n",
    "preferences, which are a type of nonlinear recursive preferences.\n",
    "\n",
    "The Bellman equation becomes\n",
    "\n",
    "$$\n",
    "v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, \n",
    "            c + \\frac{\\beta}{\\theta}\n",
    "            \\ln \\left[ \n",
    "                      \\sum_{w'} \\exp(\\theta v(w')) P(w, w')\n",
    "                \\right]\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "When $ \\theta < 0 $ the agent is risk averse.\n",
    "\n",
    "Solve the model when $ \\theta = -0.1 $ and compare your result to the risk neutral\n",
    "case.\n",
    "\n",
    "Try to interpret your result.\n",
    "\n",
    "You can start with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de08b3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "RiskModel = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c', 'θ'))\n",
    "\n",
    "def create_risk_sensitive_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "        θ=-0.1       # risk parameter\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return RiskModel(n, w_vals, P, β, c, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c933ff",
   "metadata": {},
   "source": [
    "Now you need to modify `T` and `get_greedy` and then run value function iteration again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7f3d4",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 10.1](https://jax.quantecon.org/#job_search_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03616d4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "RiskModel = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c', 'θ'))\n",
    "\n",
    "def create_risk_sensitive_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "        θ=-0.1       # risk parameter\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return RiskModel(n, w_vals, P, β, c, θ)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def T_rs(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β R v} with \n",
    "\n",
    "        e(w) = w / (1-β) and\n",
    "\n",
    "        (Rv)(w) = (1/θ) ln{E_w[ exp(θ v(W'))]}\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_greedy_rs(v, model):\n",
    "    \" Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ\n",
    "\n",
    "\n",
    "\n",
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T_rs(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy_rs(v_star, model)\n",
    "    return v_star, σ_star\n",
    "\n",
    "\n",
    "\n",
    "model_rs = create_risk_sensitive_js_model()\n",
    "\n",
    "n, w_vals, P, β, c, θ = model_rs\n",
    "\n",
    "v_star_rs, σ_star_rs = vfi(model_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19d9d9",
   "metadata": {},
   "source": [
    "Let’s plot the results together with the original risk neutral case and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109856c5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "stop_indices = jnp.where(σ_star_rs == 1)\n",
    "res_wage_index = min(stop_indices[0])\n",
    "res_wage_rs = w_vals[res_wage_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42433a6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star,  alpha=0.8, label=\"risk neutral $v$\")\n",
    "ax.plot(w_vals, v_star_rs, alpha=0.8, label=\"risk sensitive $v$\")\n",
    "ax.vlines((res_wage,), 100, 400,  ls='--', color='darkblue', \n",
    "          alpha=0.5, label=r\"risk neutral $\\bar w$\")\n",
    "ax.vlines((res_wage_rs,), 100, 400, ls='--', color='orange', \n",
    "          alpha=0.5, label=r\"risk sensitive $\\bar w$\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d993d74",
   "metadata": {},
   "source": [
    "The figure shows that the reservation wage under risk sensitive preferences (RS $ \\bar w $) shifts down.\n",
    "\n",
    "This makes sense – the agent does not like risk and hence is more inclined to\n",
    "accept the current offer, even when it’s lower."
   ]
  }
 ],
 "metadata": {
  "date": 1761799603.6230803,
  "filename": "job_search.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Job Search"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}