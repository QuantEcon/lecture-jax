{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665c0823",
   "metadata": {},
   "source": [
    "# Newton’s Method via JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89e0bf",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "This lecture was built using a machine with JAX installed and access to a GPU.\n",
    "\n",
    "To run this lecture on [Google Colab](https://colab.research.google.com/), click on the “play” icon top right, select Colab, and set the runtime environment to include a GPU.\n",
    "\n",
    "To run this lecture on your own machine, you need to install [Google JAX](https://github.com/google/jax)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9c920",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "One of the key features of JAX is automatic differentiation.\n",
    "\n",
    "While other software packages also offer this feature, the JAX version is\n",
    "particularly powerful because it integrates so closely with other core\n",
    "components of JAX, such as accelerated linear algebra, JIT compilation and\n",
    "parallelization.\n",
    "\n",
    "The application of automatic differentiation we consider is computing economic equilibria via Newton’s method.\n",
    "\n",
    "Newton’s method is a relatively simple root and fixed point solution algorithm, which we discussed\n",
    "in [a more elementary QuantEcon lecture](https://python.quantecon.org/newton_method.html).\n",
    "\n",
    "JAX is almost ideally suited to implementing Newton’s method efficiently, even\n",
    "in high dimensions.\n",
    "\n",
    "We use the following imports in this lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799788",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.optimize import root\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68556b",
   "metadata": {},
   "source": [
    "Let’s check the GPU we are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874aecbc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf12e1c",
   "metadata": {},
   "source": [
    "## Newton in one dimension\n",
    "\n",
    "As a warm up, let’s implement Newton’s method in JAX for a simple\n",
    "one-dimensional root-finding problem.\n",
    "\n",
    "Let $ f $ be a function from $ \\mathbb R $ to itself.\n",
    "\n",
    "A **root** of $ f $ is an $ x \\in \\mathbb R $ such that $ f(x)=0 $.\n",
    "\n",
    "[Recall](https://python.quantecon.org/newton_method.html) that Newton’s method for solving for the root of $ f $ involves iterating with the map $ q $ defined by\n",
    "\n",
    "$$\n",
    "q(x) = x - \\frac{f(x)}{f'(x)}\n",
    "$$\n",
    "\n",
    "Here is a function called `newton` that takes a function $ f $ plus a scalar value $ x_0 $,\n",
    "iterates with $ q $ starting from $ x_0 $, and returns an approximate fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186451d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def newton(f, x_0, tol=1e-5):\n",
    "    f_prime = jax.grad(f)\n",
    "    def q(x):\n",
    "        return x - f(x) / f_prime(x)\n",
    "\n",
    "    error = tol + 1\n",
    "    x = x_0\n",
    "    while error > tol:\n",
    "        y = q(x)\n",
    "        error = abs(x - y)\n",
    "        x = y\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1ebbe",
   "metadata": {},
   "source": [
    "The code above uses automatic differentiation to calculate $ f' $ via the call to `jax.grad`.\n",
    "\n",
    "Let’s test our `newton` routine on the function shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc69f2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "f = lambda x: jnp.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "x = jnp.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x), label='$f(x)$')\n",
    "ax.axhline(ls='--', c='k')\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.set_ylabel('$f(x)$', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245932b3",
   "metadata": {},
   "source": [
    "Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1110b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "newton(f, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b41125",
   "metadata": {},
   "source": [
    "This number looks to be close to the root, given the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd5d68",
   "metadata": {},
   "source": [
    "## An Equilibrium Problem\n",
    "\n",
    "Now let’s move up to higher dimensions.\n",
    "\n",
    "First we describe a market equilibrium problem we will solve with JAX via root-finding.\n",
    "\n",
    "The market is for $ n $ goods.\n",
    "\n",
    "(We are extending a two-good version of the market from [an earlier lecture](https://python.quantecon.org/newton_method.html).)\n",
    "\n",
    "The supply function for the $ i $-th good is\n",
    "\n",
    "$$\n",
    "q^s_i (p) = b_i \\sqrt{p_i}\n",
    "$$\n",
    "\n",
    "which we write in vector form as\n",
    "\n",
    "$$\n",
    "q^s (p) =b \\sqrt{p}\n",
    "$$\n",
    "\n",
    "(Here $ \\sqrt{p} $ is the square root of each $ p_i $ and $ b \\sqrt{p} $ is the vector\n",
    "formed by taking the pointwise product $ b_i \\sqrt{p_i} $ at each $ i $.)\n",
    "\n",
    "The demand function is\n",
    "\n",
    "$$\n",
    "q^d (p) = \\exp(- A p) + c\n",
    "$$\n",
    "\n",
    "(Here $ A $ is an $ n \\times n $ matrix containing parameters, $ c $ is an $ n \\times\n",
    "1 $ vector and the $ \\exp $ function acts pointwise (element-by-element) on the\n",
    "vector $ - A p $.)\n",
    "\n",
    "The excess demand function is\n",
    "\n",
    "$$\n",
    "e(p) = \\exp(- A p) + c - b \\sqrt{p}\n",
    "$$\n",
    "\n",
    "An **equilibrium price** vector is an $ n $-vector $ p $ such that $ e(p) = 0 $.\n",
    "\n",
    "The function below calculates the excess demand for given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23067496",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def e(p, A, b, c):\n",
    "    return jnp.exp(- A @ p) + c - b * jnp.sqrt(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e10e07",
   "metadata": {},
   "source": [
    "## Computation\n",
    "\n",
    "In this section we describe and then implement the solution method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31791a3a",
   "metadata": {},
   "source": [
    "### Newton’s Method\n",
    "\n",
    "We use a multivariate version of Newton’s method to compute the equilibrium price.\n",
    "\n",
    "The rule for updating a guess $ p_n $ of the equilibrium price vector is\n",
    "\n",
    "\n",
    "<a id='equation-multi-newton'></a>\n",
    "$$\n",
    "p_{n+1} = p_n - J_e(p_n)^{-1} e(p_n) \\tag{3.1}\n",
    "$$\n",
    "\n",
    "Here $ J_e(p_n) $ is the Jacobian of $ e $ evaluated at $ p_n $.\n",
    "\n",
    "Iteration starts from initial guess $ p_0 $.\n",
    "\n",
    "Instead of coding the Jacobian by hand, we use automatic differentiation via `jax.jacobian()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2fb55",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def newton(f, x_0, tol=1e-5, max_iter=15):\n",
    "    \"\"\"\n",
    "    A multivariate Newton root-finding routine.\n",
    "\n",
    "    \"\"\"\n",
    "    x = x_0\n",
    "    f_jac = jax.jacobian(f)\n",
    "    @jax.jit\n",
    "    def q(x):\n",
    "        \" Updates the current guess. \"\n",
    "        return x - jnp.linalg.solve(f_jac(x), f(x))\n",
    "    error = tol + 1\n",
    "    n = 0\n",
    "    while error > tol:\n",
    "        n += 1\n",
    "        if(n > max_iter):\n",
    "            raise Exception('Max iteration reached without convergence')\n",
    "        y = q(x)\n",
    "        error = jnp.linalg.norm(x - y)\n",
    "        x = y\n",
    "        print(f'iteration {n}, error = {error}')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc8fc3",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Let’s now apply the method just described to investigate a large market with 5,000 goods.\n",
    "\n",
    "We randomly generate the matrix $ A $ and set the parameter vectors $ b, c $ to $ 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9bc9f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "dim = 5_000\n",
    "seed = 32\n",
    "\n",
    "# Create a random matrix A and normalize the rows to sum to one\n",
    "key = jax.random.PRNGKey(seed)\n",
    "A = jax.random.uniform(key, [dim, dim])\n",
    "s = jnp.sum(A, axis=0)\n",
    "A = A / s\n",
    "\n",
    "# Set up b and c\n",
    "b = jnp.ones(dim)\n",
    "c = jnp.ones(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4b99f",
   "metadata": {},
   "source": [
    "Here’s our initial condition $ p_0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed58bb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "init_p = jnp.ones(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a4ff9",
   "metadata": {},
   "source": [
    "By combining the power of Newton’s method, JAX accelerated linear algebra,\n",
    "automatic differentiation, and a GPU, we obtain a relatively small error for\n",
    "this high-dimensional problem in just a few seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e772a5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "p = newton(lambda p: e(p, A, b, c), init_p).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b33cba",
   "metadata": {},
   "source": [
    "Here’s the size of the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8cbb4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "jnp.max(jnp.abs(e(p, A, b, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f00bd",
   "metadata": {},
   "source": [
    "With the same tolerance, SciPy’s `root` function takes much longer to run,\n",
    "even with the Jacobian supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41930c93",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "solution = root(lambda p: e(p, A, b, c),\n",
    "                init_p,\n",
    "                jac=lambda p: jax.jacobian(e)(p, A, b, c),\n",
    "                method='hybr',\n",
    "                tol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451583c9",
   "metadata": {},
   "source": [
    "The result is also slightly less accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6567a58",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "p = solution.x\n",
    "jnp.max(jnp.abs(e(p, A, b, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a215f",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66730d",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Consider a three-dimensional extension of [the Solow fixed point\n",
    "problem](https://python.quantecon.org/newton_method.html#the-solow-model) with\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "            2 & 3 & 3 \\\\\n",
    "            2 & 4 & 2 \\\\\n",
    "            1 & 5 & 1 \\\\\n",
    "        \\end{pmatrix},\n",
    "            \\quad\n",
    "s = 0.2, \\quad α = 0.5, \\quad δ = 0.8\n",
    "$$\n",
    "\n",
    "As before the law of motion is\n",
    "\n",
    "$$\n",
    "k_{t+1} = g(k_t) \\quad \\text{where} \\quad\n",
    "    g(k) := sAk^\\alpha + (1-\\delta) k\n",
    "$$\n",
    "\n",
    "However $ k_t $ is now a $ 3 \\times 1 $ vector.\n",
    "\n",
    "Solve for the fixed point using Newton’s method with the following initial values:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    k1_{0} &= (1, 1, 1) \\\\\n",
    "    k2_{0} &= (3, 5, 5) \\\\\n",
    "    k3_{0} &= (50, 50, 50)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- The computation of the fixed point is equivalent to computing $ k^* $ such that $ f(k^*) - k^* = 0 $.  \n",
    "- If you are unsure about your solution, you can start with the solved example:  \n",
    "\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "            2 & 0 & 0 \\\\\n",
    "            0 & 2 & 0 \\\\\n",
    "            0 & 0 & 2 \\\\\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with $ s = 0.3 $, $ α = 0.3 $, and $ δ = 0.4 $ and starting value:\n",
    "\n",
    "$$\n",
    "k_0 = (1, 1, 1)\n",
    "$$\n",
    "\n",
    "The result should converge to the [analytical solution](https://python.quantecon.org/newton_method.html#solved-k)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d5698",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 3.1](https://jax.quantecon.org/#newton_ex1)\n",
    "\n",
    "Let’s first define the parameters for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac25b48",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "A = jnp.array([[2.0, 3.0, 3.0],\n",
    "               [2.0, 4.0, 2.0],\n",
    "               [1.0, 5.0, 1.0]])\n",
    "s = 0.2\n",
    "α = 0.5\n",
    "δ = 0.8\n",
    "initLs = [jnp.ones(3),\n",
    "          jnp.array([3.0, 5.0, 5.0]),\n",
    "          jnp.repeat(50.0, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3da13",
   "metadata": {},
   "source": [
    "Then we define the multivariate version of the formula for the [law of motion of capital](https://python.quantecon.org/newton_method.html#solow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa04af",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def multivariate_solow(k, A=A, s=s, α=α, δ=δ):\n",
    "    return s * jnp.dot(A, k**α) + (1 - δ) * k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082a5de",
   "metadata": {},
   "source": [
    "Let’s run through each starting value and see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2fa4e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "attempt = 1\n",
    "for init in initLs:\n",
    "    print(f'Attempt {attempt}: Starting value is {init} \\n')\n",
    "    %time k = newton(lambda k: multivariate_solow(k) - k, \\\n",
    "                     init).block_until_ready()\n",
    "    print('-'*64)\n",
    "    attempt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65771ef2",
   "metadata": {},
   "source": [
    "We find that the results are invariant to the starting values.\n",
    "\n",
    "But the number of iterations it takes to converge is dependent on the starting values.\n",
    "\n",
    "Let substitute the output back into the formulate to check our last result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ff3ec",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "multivariate_solow(k) - k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455e53e",
   "metadata": {},
   "source": [
    "Note the error is very small.\n",
    "\n",
    "We can also test our results on the known solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec8216",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "A = jnp.array([[2.0, 0.0, 0.0],\n",
    "               [0.0, 2.0, 0.0],\n",
    "               [0.0, 0.0, 2.0]])\n",
    "s = 0.3\n",
    "α = 0.3\n",
    "δ = 0.4\n",
    "init = jnp.repeat(1.0, 3)\n",
    "%time k = newton(lambda k: multivariate_solow(k, A=A, s=s, α=α, δ=δ) - k, \\\n",
    "                 init).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f10f8",
   "metadata": {},
   "source": [
    "The result is very close to the true solution but still slightly different.\n",
    "\n",
    "We can increase the precision of the floating point numbers and restrict the tolerance to obtain a more accurate approximation (see detailed discussion in the [lecture on JAX](https://python-programming.quantecon.org/jax_intro.html#differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf46e5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# We will use 64 bit floats with JAX in order to increase the precision.\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "init = init.astype('float64')\n",
    "\n",
    "%time k = newton(lambda k: multivariate_solow(k, A=A, s=s, α=α, δ=δ) - k,\\\n",
    "                 init,\\\n",
    "                 tol=1e-7).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4463252",
   "metadata": {},
   "source": [
    "We can see it steps towards a more accurate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0752a",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "\n",
    "In this exercise, let’s try different initial values and check how Newton’s method responds to different starting points.\n",
    "\n",
    "Let’s define a three-good problem with the following default values:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "            0.2 & 0.1 & 0.7 \\\\\n",
    "            0.3 & 0.2 & 0.5 \\\\\n",
    "            0.1 & 0.8 & 0.1 \\\\\n",
    "        \\end{pmatrix},\n",
    "            \\qquad\n",
    "b = \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            1 \\\\\n",
    "            1\n",
    "        \\end{pmatrix}\n",
    "    \\qquad \\text{and} \\qquad\n",
    "c = \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            1 \\\\\n",
    "            1\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For this exercise, use the following extreme price vectors as initial values:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    p1_{0} &= (5, 5, 5) \\\\\n",
    "    p2_{0} &= (1, 1, 1) \\\\\n",
    "    p3_{0} &= (4.5, 0.1, 4)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Set the tolerance to $ 10^{-15} $ for more accurate output.\n",
    "\n",
    "Similar to [exercise 1](#newton_ex1), enabling `float64` for JAX can improve the precision of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2bf0b",
   "metadata": {},
   "source": [
    "## Solution to[ Exercise 3.2](https://jax.quantecon.org/#newton_ex2)\n",
    "\n",
    "Define parameters and initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3f70b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "A = jnp.array([\n",
    "    [0.2, 0.1, 0.7],\n",
    "    [0.3, 0.2, 0.5],\n",
    "    [0.1, 0.8, 0.1]\n",
    "])\n",
    "b = jnp.array([1.0, 1.0, 1.0])\n",
    "c = jnp.array([1.0, 1.0, 1.0])\n",
    "initLs = [jnp.repeat(5.0, 3),\n",
    "          jnp.array([4.5, 0.1, 4.0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0e404",
   "metadata": {},
   "source": [
    "Let’s run through each initial guess and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac339c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "attempt = 1\n",
    "for init in initLs:\n",
    "    print(f'Attempt {attempt}: Starting value is {init} \\n')\n",
    "    init = init.astype('float64')\n",
    "    %time p = newton(lambda p: e(p, A, b, c), \\\n",
    "                 init, \\\n",
    "                 tol=1e-15, max_iter=15).block_until_ready()\n",
    "    print('-'*64)\n",
    "    attempt +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6b555",
   "metadata": {},
   "source": [
    "We can find that Newton’s method may fail for some starting values.\n",
    "\n",
    "Sometimes it may take a few initial guesses to achieve convergence.\n",
    "\n",
    "Substitute the result back to the formula to check our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20727a7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "e(p, A, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9579050",
   "metadata": {},
   "source": [
    "We can see the result is very accurate."
   ]
  }
 ],
 "metadata": {
  "date": 1714699246.4214895,
  "filename": "newtons_method.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Newton’s Method via JAX"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}