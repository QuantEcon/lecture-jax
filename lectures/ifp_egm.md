---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.5
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

+++ {"user_expressions": []}

# Endogenous Grid Method


```{include} _admonition/gpu.md
```

## Overview

In this lecture we use the endogenous grid method (EGM) to solve a basic income
fluctuation (optimal savings) problem.

Some general backgroud on The endogeous grid method can be found in [an earlier
QuantEcon lecture](https://python.quantecon.org/egm_policy_iter.html).

Here we focus on providing an efficient JAX implmentation.

We will use the following libraries and imports.

```{code-cell} ipython3
:tags: [hide-output]

!pip install --upgrade quantecon interpolation
```

```{code-cell} ipython3
import quantecon as qe
import matplotlib.pyplot as plt
import numpy as np
import jax
import jax.numpy as jnp

from interpolation import interp
from numba import njit, float64
from numba.experimental import jitclass
```

+++ {"user_expressions": []}

We use 64 bit floating point numbers for extra precision.

```{code-cell} ipython3
jax.config.update("jax_enable_x64", True)
```

+++ {"user_expressions": []}

## Setup 

We consider a household that chooses a state-contingent consumption plan $\{c_t\}_{t \geq 0}$ to maximize

$$
\mathbb{E} \, \sum_{t=0}^{\infty} \beta^t u(c_t)
$$

subject to

$$
    a_{t+1} \leq  R(a_t - c_t)  + Y_{t+1},
    \quad c_t \geq 0,
    \quad a_t \geq 0
    \quad t = 0, 1, \ldots
$$

Here $R = 1 + r$ where $r$ is the interest rate.

The income process $\{Y_t\}$ is a [Markov chain](https://python.quantecon.org/finite_markov.html) generated by stochastic matrix $P$.

The matrix $P$ and the grid of values taken by $Y_t$ are obtained by
discretizing the AR(1) process

$$
    Y_{t+1} = \rho Y_t + \nu \epsilon_{t+1}
$$

where $\{\epsilon_t\}$ is IID and standard normal.

Utility has the CRRA specification

$$
    u(c) = \frac{c^{1 - \gamma}} {1 - \gamma}
$$


The following function stores default parameter values for the income
fluctuation problem and creates suitable arrays.

```{code-cell} ipython3
def ifp(R=1.01,             # gross interest rate
        β=0.96,             # discount factor
        γ=1.5,              # CRRA preference parameter
        grid_max=16,        # asset grid max
        grid_size=200,      # asset grid size
        ρ=0.99,             # income persistence
        ν=0.02,             # income volatility
        y_size=25):         # income grid size
  
    # Create income Markov chain
    mc = qe.tauchen(y_size, ρ, ν)
    y_grid, P = jnp.exp(mc.state_values), mc.P
    # Shift to JAX arrays
    P, y_grid = jax.device_put((P, y_grid))
    
    a_grid = jnp.linspace(0, grid_max, grid_size)
    sizes = grid_size, y_size
    a_grid, y_grid, P = jax.device_put((a_grid, y_grid, P))

    # require R β < 1 for convergence
    assert R * β < 1, "Stability condition violated."
        
    return (β, R, γ), sizes, (a_grid, y_grid, P)
```

+++ {"user_expressions": []}

## Solution method

Let $S = \mathbb R_+ \times \mathsf Y$ be the set of possible values for the state $(a_t, Y_t)$.

We aim to compute an optimal consumption policy $\sigma^* \colon S \to \mathbb R$, under which dynamics are given by

$$
    (a_0, y_0) = (a, y),
    \quad
    c_t = \sigma^*(a_t, Y_t)
    \quad \text{and} \quad
    a_{t+1} = R (a_t - c_t) + Y_{t+1}
$$


In this section we discuss how we intend to solve for this policy.


### Euler equation

The Euler equation for the optimization problem is

$$
    u' (c_t)
    = \max \left\{
        \beta R \,  \mathbb{E}_t  u'(c_{t+1})  \,,\;  u'(a_t)
    \right\}
$$

An explanation for this expression can be found [here](https://python.quantecon.org/ifp.html#value-function-and-euler-equation).

We rewrite the Euler equation in functional form

$$
    (u' \circ \sigma)  (a, y)
    = \max \left\{
    \beta R \, \mathbb E_y (u' \circ \sigma)
        [R (a - \sigma(a, y)) + \hat Y, \, \hat Y]
    \, , \;
         u'(a)
         \right\}
$$


where $(u' \circ \sigma)(a, y) := u'(\sigma(a, y))$ and $\sigma$ is a consumption
policy.

For given consumption policy $\sigma$, we define $(K \sigma) (a,y)$ as the unique $c \in [0, a]$ that solves

$$
u'(c)
= \max \left\{
           \beta R \, \mathbb E_y (u' \circ \sigma) \,
           [R (a - c) + \hat Y, \, \hat Y]
           \, , \;
           u'(a)
     \right\}
$$ (eq:kaper)

It [can be shown that](https://python.quantecon.org/ifp.html)

1. iterating with $K$ computes an optimal policy and
2. if $\sigma$ is increasing in its first argument, the so is $K\sigma$

Hence below we always assume that $\sigma$ is increasing in its first argument.

The EGM is a method for computing the update $K\sigma$ given $\sigma$ along a grid of asset values.

Notice that, since $u'(a) \to \infty$ as $a \downarrow 0$, the second term in
the max in [](eq:kaper) dominates for sufficiently small $a$.

Also, again using [](eq:kaper), we have $c=a$ for all such $a$.

Hence, for sufficiently small $a$,

$$
   u'(a) \geq
   \beta R \, \mathbb E_y (u' \circ \sigma) \,
           [\hat Y, \, \hat Y]
$$

Equality holds at $\bar a(y)$ given by 

$$
   \bar a (y) =
   (u')^{-1}
   \left\{
       \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [\hat Y, \, \hat Y]
   \right\}
$$

We can now write

$$
u'(c)
    = \begin{cases}
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R (a - c) + \hat Y, \, \hat Y]
               & \text{if } a > \bar a (y) \\
        u'(a)  & \text{if } a \leq \bar a (y)
    \end{cases}
$$

Equivalently, we can state that the $c$ satisfying $c = (K\sigma)(a, y)$ obeys

$$
c = \begin{cases}
        (u')^{-1}
        \left\{
            \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R (a - c) + \hat Y, \, \hat Y]
        \right\}
               & \text{if } a > \bar a (y) \\
            a  & \text{if } a \leq \bar a (y)
    \end{cases}
$$ (eq:oro)

We begin with an *exogenous* grid of saving values $0 = s_0 < \ldots < s_{N-1}$

Using the exogenous savings grid, and a fixed value of $y$, we create an *endogenous* asset grid
$a_0, \ldots, a_{N-1}$ and a consumption grid $c_0, \ldots, c_{N-1}$ as follows.

First we set $a_0 = c_0 = 0$, since zero consumption is an optimal (in fact the only) choice when $a=0$.

Then, for $i > 0$, we compute 

$$
    c_i
    = (u')^{-1}
    \left\{ 
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R s_i + \hat Y, \, \hat Y]
     \right\}
     \quad \text{for all } i
$$ (eq:kaperc)

and we set 

$$
    a_i = s_i + c_i 
$$ 

We claim that each pair $a_i, c_i$ obeys [](eq:oro).

Indeed, since $s_i > 0$, choosing $c_i$ according to [](eq:kaperc) gives

$$
    c_i
    = (u')^{-1}
    \left\{ 
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R s_i + \hat Y, \, \hat Y]
     \right\}
     \geq \bar a(y)
$$

where the inequality uses the fact that $\sigma$ is increasing in its first argument.

If we now take $a_i = s_i + c_i$ we get $a_i > \bar a(y)$, so the pair $(a_i, c_i)$ obeys
$$
    c_i
    = (u')^{-1}
    \left\{ 
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R (a_i - c_i) + \hat Y, \, \hat Y]
     \right\}
     \quad \text{and} \quad a_i > \bar a(y)
$$


Hence [](eq:oro) holds.


Finally, we know that $a=0$ implies $c=0$, so we append the points $a_0 = 0$ and $c_0 = 0$ to the endogenous grids.


### JAX version 

First we define a vectorized operator $K$ based on [](eq:kaper).

```{code-cell} ipython3
def K_egm(a_in, σ_in, constants, sizes, arrays):
    """
    The vectorzied operator K using EGM.

    """
    
    # Unpack
    β, R, γ = constants
    grid_size, y_size = sizes
    grid, y_grid, P = arrays
    
    def u_prime(c):
        return c**(-γ)

    def u_prime_inv(u_prime):
            return u_prime**(-1/γ)


    # Linearly interpolate σ(a, z)
    def σ(a, z):
        return jnp.interp(a, a_in[:,z], σ_in[:,z])

    σ_vec = jnp.vectorize(σ)

    
    # Broadcast and vectorize
    z_grid = jnp.reshape(jnp.arange(y_size), (1, y_size, 1))
    z_hat_grid = jnp.reshape(jnp.arange(y_size), (1, 1, y_size))
    s_grid = jnp.reshape(grid, (grid_size, 1, 1))
    
    # Evaluate σ_out
    a_next = R * s_grid + y_grid[z_hat_grid]
    σ_next = σ_vec(a_next, z_hat_grid)
    up = u_prime(σ_next)
    P = jnp.reshape(P, (1, y_size, y_size))
    E = jnp.sum(up * P, axis=-1)

    σ_out = u_prime_inv(β * R * E)

    # Compute a_out by s = a - c
    a_out = grid[:, jnp.newaxis] + σ_out
    
    # Set σ_0 = 0 and a_0 = 0
    σ_out = σ_out.at[0, :].set(0)
    a_out = a_out.at[0, :].set(0)

    return a_out, σ_out
```

+++ {"user_expressions": []}

Then we use ``jax.jit`` to compile $K$.

```{code-cell} ipython3
K_egm_jax = jax.jit(K_egm, static_argnums=(3,))
```

+++ {"user_expressions": []}

Next we define a successive approximator that repeatedly applies $K$.

```{code-cell} ipython3
def successive_approx_jax(model,        
            tol=1e-5,
            max_iter=1000,
            verbose=True,
            print_skip=25):

    # Unpack
    constants, sizes, arrays = model
    
    β, R, γ = constants
    grid_size, y_size = sizes
    grid, y_grid, P = arrays
    
    # Set up loop
    σ_init = jnp.tile(grid, [y_size, 1]).T
    a_init = jnp.copy(σ_init)
    a_vec, σ_vec = a_init, σ_init
    
    i = 0
    error = tol + 1

    while i < max_iter and error > tol:
        a_new, σ_new = K_egm_jax(a_vec, σ_vec, constants, sizes, arrays)    
        error = jnp.max(jnp.abs(σ_vec - σ_new))
        i += 1
        if verbose and i % print_skip == 0:
            print(f"Error at iteration {i} is {error}.")
        a_vec, σ_vec = jnp.copy(a_new), jnp.copy(σ_new)

    if error > tol:
        print("Failed to converge!")
    elif verbose:
        print(f"\nConverged in {i} iterations.")

    return a_new, σ_new
```

+++ {"user_expressions": []}

### Numba version 

Below we provide a second set of code, which solves the same model with Numba.

The purpose of this code is to cross-check our results from the JAX version, as
well as to do a runtime comparison.

Most readers will want to skip ahead to the next section, where we solve the
model and run the cross-check.

```{code-cell} ipython3
ifp_data = [
    ('R', float64),              
    ('β', float64),             
    ('γ', float64),            
    ('P', float64[:, :]),     
    ('y_grid', float64[:]),  
    ('grid', float64[:])    
]

# Use the JAX IFP data as our defaults for the Numba version
model = ifp()
constants, sizes, arrays = model
β, R, γ = constants
grid_size, y_size = sizes
grid, y_grid, P = (np.array(a) for a in arrays)

@jitclass(ifp_data)
class IFP:

    def __init__(self,
                 R=R,
                 β=β,
                 γ=γ,
                 P=np.array(P),
                 y_grid=np.array(y_grid),
                 grid=grid):

        self.R, self.β, self.γ = R, β, γ
        self.P, self.y_grid = P, y_grid
        self.grid = grid

        # Recall that we need R β < 1 for convergence.
        assert self.R * self.β < 1, "Stability condition violated."

    def u_prime(self, c):
        return c**(-self.γ)
    
    def u_prime_inv(self, u_prime):
        return u_prime**(-1/self.γ)
```

```{code-cell} ipython3
@njit
def K_egm_nb(a_in, σ_in, ifp):
    """
    The operator K using Numba.

    """
    
    # Simplify names
    R, P, y_grid, β, γ  = ifp.R, ifp.P, ifp.y_grid, ifp.β, ifp.γ
    grid, u_prime = ifp.grid, ifp.u_prime
    u_prime_inv = ifp.u_prime_inv
    n = len(y_grid)
    
    # Linear interpolation of policy using endogenous grid
    def σ(a, z):
        return interp(a_in[:, z], σ_in[:, z], a)
    
    # Allocate memory for new consumption array
    σ_out = np.zeros_like(σ_in)
    a_out = np.zeros_like(σ_out)
    
    for i, s in enumerate(grid[1:]):
        i += 1
        for z in range(n):
            expect = 0.0
            for z_hat in range(n):
                expect += u_prime(σ(R * s + y_grid[z_hat], z_hat)) * P[z, z_hat]

            c = u_prime_inv(β * R * expect)
            σ_out[i, z] = c
            a_out[i, z] = s + c
    
    return a_out, σ_out
```

```{code-cell} ipython3
def successive_approx_numba(model,        # Class with model information
              tol=1e-5,
              max_iter=1000,
              verbose=True,
              print_skip=25):

    # Unpack
    P, grid = model.P, model.grid
    n = len(P)
    
    σ_init = np.tile(grid, (n, 1)).T
    a_init = np.copy(σ_init)
    a_vec, σ_vec = a_init, σ_init
    
    # Set up loop
    i = 0
    error = tol + 1

    while i < max_iter and error > tol:
        a_new, σ_new = K_egm_nb(a_vec, σ_vec, model)
        error = np.max(np.abs(σ_vec - σ_new))
        i += 1
        if verbose and i % print_skip == 0:
            print(f"Error at iteration {i} is {error}.")
        a_vec, σ_vec = np.copy(a_new), np.copy(σ_new)

    if error > tol:
        print("Failed to converge!")
    elif verbose:
        print(f"\nConverged in {i} iterations.")

    return a_new, σ_new
```

+++ {"user_expressions": []}

## Solutions

First we solve the IFP with JAX.

```{code-cell} ipython3
ifp_jax = ifp()
```

+++ {"user_expressions": []}

Here's a first run.

```{code-cell} ipython3
qe.tic()
a_star_egm_jax, σ_star_egm_jax = successive_approx_jax(ifp_jax,
                                         print_skip=5)
qe.toc()
```

+++ {"user_expressions": []}

Let's run again so we can see the run time without compilation.

```{code-cell} ipython3
qe.tic()
a_star_egm_jax, σ_star_egm_jax = successive_approx_jax(ifp_jax,
                                         print_skip=5)
jax_time = qe.toc()
```

+++ {"user_expressions": []}

Next let's solve the same IFP with Numba.

```{code-cell} ipython3
ifp_numba = IFP()
```

```{code-cell} ipython3
qe.tic()
a_star_egm_nb, σ_star_egm_nb = successive_approx_numba(ifp_numba,
                                         print_skip=5)
qe.toc()
```

+++ {"user_expressions": []}

Let's run again so we can see the run time without compilation.

```{code-cell} ipython3
qe.tic()
a_star_egm_nb, σ_star_egm_nb = successive_approx_numba(ifp_numba,
                                         print_skip=5)
numba_time = qe.toc()
```

+++ {"user_expressions": []}

The JAX code is significantly faster, as expected.

This difference will increase when more features (and state variables) are added
to the model.

Lastly, let's plot the consumption functions for a sanity check.

```{code-cell} ipython3
fig, ax = plt.subplots()

n = len(ifp_numba.P)
for z in (0, y_size-1):
    ax.plot(a_star_egm_nb[:, z], 
            σ_star_egm_nb[:, z], 
            '--', lw=2,
            label=f"Numba EGM: consumption when $z={z}$")
    ax.plot(a_star_egm_jax[:, z], 
            σ_star_egm_jax[:, z], 
            label=f"JAX EGM: consumption when $z={z}$")

ax.set_xlabel('asset')
plt.legend()
plt.show()
```

```{code-cell} ipython3

```

```{code-cell} ipython3

```
